{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"learn-env","language":"python","name":"learn-env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"YouTube_Scraping.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"korean-tattoo"},"source":["import os\n","import googleapiclient.discovery\n","from googleapiclient.errors import HttpError\n","from datetime import datetime\n","import pandas as pd\n","import requests\n","import time "],"id":"korean-tattoo","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ongoing-malpractice"},"source":["youtube = googleapiclient.discovery.build(\n","    \"youtube\", \"v3\",\n","    developerKey = )"],"id":"ongoing-malpractice","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mounted-research"},"source":["def get_comments_from_video(video_id: str, max_comments = 0) -> None:\n","    \"\"\"\n","    Takes the video's ID, the string at the end of the url, after \"v=\",\n","    and compiles the comments, appending them to a global dataframe df.\n","\n","    Optionally accepts a limit to comments gathered, default value of\n","    zero returns all comments. If you specify a max, you may still get\n","    more than specified due to how the API responds.\n","\n","    Comments are returned with the most recently interacted\n","    with first, such as a brand new comment, or an old comment with a new\n","    reply added. \n","    \"\"\"\n","    # Define a function to reset dict_ with, to facilitate appending\n","    def reset_dict():\n","        return {\n","            'video_id':[], 'text': [], 'likes': [],\n","            'date': [], 'channel_id': [], 'viewer_rating':[],\n","            'mentions':[], 'comment_id':[]\n","            }\n","    # Initialize the dict_ and access the global df reference\n","    dict_ = reset_dict()\n","    global df\n","\n","    # Adds data from the comment thread's top comment to the dict_\n","    def read_top_level_comment(comment):\n","        snip = comment['snippet']['topLevelComment']['snippet']\n","        dict_['video_id'].append(['videoId'])\n","        dict_['text'].append(snip['textOriginal'])\n","        dict_['likes'].append(snip['likeCount'])\n","        dict_['date'].append(snip['publishedAt'])\n","        dict_['channel_id'].append(snip['authorChannelId']['value'])\n","        dict_['viewer_rating'].append(snip['viewerRating'])\n","        dict_['mentions'].append('')\n","        dict_['comment_id'].append(comment['id'])\n","\n","    # Adds the relevent data to a reply comment to the dict_\n","    # Is a seperate function, the data isnt located in identical locations\n","    def read_reply(reply):\n","        # Reply comments do not carry their own videoId reference\n","        snip = reply['snippet']\n","        dict_['video_id'].append(dict_['video_id'][-1])\n","        dict_['text'].append(snip['textOriginal'])\n","        dict_['likes'].append(snip['likeCount'])\n","        dict_['date'].append(snip['publishedAt'])\n","        dict_['channel_id'].append(snip['authorChannelId']['value'])\n","        dict_['viewer_rating'].append(snip['viewerRating'])\n","        dict_['mentions'].append(snip['parentId'])\n","        dict_['comment_id'].append(reply['id'])\n","\n","    # Determine if a max number of comments\n","    # is called for and create initial request\n","    max_comments = max_comments if max_comments > 0 else float('inf')\n","    request = youtube.commentThreads().list(\n","        part = \"snippet,replies\",\n","        videoId = video_id,\n","        maxResults = min(100, max_comments))\n","    \n","    # Loop persists until all comments gathered/max is exceeded\n","    while request is not None and max_comments > 0:\n","        # Try most recent request, ending function if an error occurs\n","        try:\n","            response = request.execute()\n","        except HttpError as err:\n","            print(err)\n","            return\n","\n","        # After positive response, loop through top level comments\n","        for comment_thread in response['items']:\n","            read_top_level_comment(comment_thread)\n","            # If no replies to the top level comment, skip\n","            if comment_thread['snippet']['totalReplyCount'] < 1:\n","                continue\n","            # If there are up to five replies, they are all included in \n","            # the original response and can be read before moving on\n","            if comment_thread['snippet']['totalReplyCount'] <= 5:\n","                for reply in comment_thread['replies']['comments']:\n","                    read_reply(reply)\n","                continue\n","            # Finally, if there are more than five replies, \n","            # a new request must be made to retrieve them all\n","            reply_req = youtube.comments().list(\n","                part = \"snippet\",\n","                parentId = comment_thread['id'],\n","                maxResults = 100)\n","            # A new loop is necessary in case they exceed 100\n","            while reply_req is not None:\n","                # Try most recent reply request, breaking the loop if\n","                # HttpError. This loop might cause you to exceed your\n","                #  daily limit, so we will just move on from errors\n","                try:\n","                    reply_resp = reply_req.execute()\n","                except HttpError as err:\n","                    break\n","                for reply in reply_resp['items']:\n","                    read_reply(reply)\n","                reply_req = youtube.comments().list_next(reply_req,\n","                                                        reply_resp)\n","\n","        # Decrement max and request next page if possible/needed\n","        max_comments -= len(dict_['text'])\n","        request = youtube.commentThreads().list_next(request, response)\n","\n","        # Append the dataframe with the comments\n","        #  gathered in this loop and reset the dict for the next loop\n","        df = df.append(pd.DataFrame.from_dict(dict_))\n","        df = df.reset_index(drop=True)\n","        dict_ = reset_dict()"],"id":"mounted-research","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"extraordinary-stranger"},"source":["# making a list containing all of the videos we've scraped so that we don't waste any time re-scraping them \n","completed_videos =   ['JWeR_F4uyE0', 'VIMV6E8OxG8', 'THqtAQOicQI', \n","                      '6VBCxWcAPXw', 'PQnvjGN91Mg', 'Xj1tzy_lTyU', \n","                      'kmFOBoy2MZ8', '76sJ7C0QEJs', 'C30gxc6TWuY', \n","                      'W9olSzNOh8s', 'kS0Jg6hlUSs', 't9c7aheZxls',\n","                      't_n0yhhuJBs', 'NtQkz0aRDe8', '-9lBVznUuHk', \n","                      'X8bBP_cLrl0', 'b3D7QlMVa5s', 'KQqHDEYpIvI',\n","                      'wYDJ0vxg1lU', 'NtQkz0aRDe8', '-dL28N5yPmQ', \n","                      'R_LqgcndmAo', 'eH-xm9G9QBk', 'DMMPYkRrd4o'\n","                      'eXRdZ_qnZTA', '2emC9xPKh_Q', 'h8T9mVkGh3s', \n","                      'gWKyTYEFVGY', '-YebEDmbG_M', 'llh8rfwWqqY', \n","                      '60V0_-AHfyM', 'w6J7FteaW2Q', 'H0CBLw0xOlo', \n","                      'QKq4sLERZ4M', '9T6WqdHq7JY', 'kS1J-ZSaecw', \n","                      'w_4D6xKqH9w', 'D1KPZOK-iHg', 'ji5i4gXBcSk', \n","                      'LdIW_bOaspg', 'AKLnXeFDQ1A', 'TOivsknjD0k', \n","                      'njESY1JxNcM', 'P5BNNA97LEc', 'VOD_uugAlJw', \n","                      'xg_jyUDsLpU', 'N_SjGaiuGoU', '1xWbCcaJnIQ', \n","                      'db4cEuLpPsQ', 'aTci511TD4A', 'ego91VOyObw', \n","                      '88QDCJkNLlE', 'LOkqR4CK7Qc', 'hgE-v1OEJFM', \n","                      'hzp7vqgprCc', 'uFhsagtKtwM', 'QKq4sLERZ4M', \n","                      'JzeYsRt7axc', 'nhimQHsTo0s', '8ydvxFu6bJ8', \n","                      '9ot3bCkhjTM', 'mKAIL8DDemg', 'kPd56OY2ED8', \n","                      'FTcXKFZcToM', '-R2x02n-o64', 'vS7aidy2bwk', \n","                      'iB0ilH7yrfU', 'XVqPwcnRGBU', 'OyrFddzsymQ', \n","                      '0kZ-EcGt39s', '0pGzSKohRJo', 'e7o4ct0Z8tI',\n","                      'VYD0DleJn7U', 'OYAgcS31-p0', 'Zo62S0ulqhA', \n","                      '5OLtteIwwNs', 'ZL4yYHdDSWs', 'UNEFDynNw-Q',\n","                      't59Ge4O70iM', 'AhF44UT2AIk', 'biSWmzIg-2k', \n","                      'N-1gzo3Pyvo', '0kZ-EcGt39s', 'OyF1ByhjSv0', \n","                      'g_ROkapCj14', 'Bsgrbd_Yv4Y', 'fXcmzmWXZmw', \n","                      'fzyYAVz3IGg', 'ep3GlvxUUew', '2QI7z46LWLY', \n","                      '8Oh5ARY_MCM', 'fniq8Wuw60A', 'H0CBLw0xOlo', \n","                      '5nE3UO1kqv0', '-nwbLls-PCs', 'vTNP01Sg-Ss', \n","                      'PHY_vAKLzzo', 'BpPmP8DUh4s', 'F-c5iAyfgAU', \n","                      'lQS8cnsZuGU', 'LmfaVwAXZy4', 'wtlUnI1fe8Y', \n","                      '35b2tAMxQXg', 'hTbQF6UBe5Q', 'IW9A-uWM0JU', \n","                      '85vvVZ4jSZM', 'xudplVZgGV0', '76sJ7C0QEJs', \n","                      '8Uvgh4gYzlw', 'ySKIm7k1-18', 'oAqhNmLmY7g', \n","                      'C30gxc6TWuY', '1iGriklFHHQ', 'GQ7v2dI2RF4', \n","                      'lCCKdcL_h3Y', 'oBXmUP3Jq8A', 'SM1vXb6J7gE', \n","                      'dTEIL19FLYI', 'VZpN7hd1ybI', 'C9GiZDoZvxE',\n","                      '-qov7HlrvbM', 'KZXjFdrct-w', 'NyLPPXaGl5A', \n","                      'C2jh7dCwGRs', 'XL1ehbG9EL8', '42Je9Xczu0o', \n","                      'D-J9maAnhwg', '_Ihdb8-h5Ek', '4cv3SjVK-n0', \n","                      'hYyg8JC-6ew', 'RcXBuYwm3xk', '-YebEDmbG_M', \n","                      'TNRQFKVV68I', 'pxa0IrZCNzg', 'vFdx1Hs71iA', \n","                      'fM-JHvg-ZCM', 'aCCR5qBsD0c', 'cb6sdimG8GE', \n","                      '0ENabNTQwNg', 'LqoYtBZAKO0', 'H2f0Wd3zNj0', \n","                      'JkeLIAd2Nd0', 'TmLWxptFFYc', 'S0dqd72ALkQ', \n","                      '0Ap4JhPoPQY', 'P4aXmnQzJ0o', 'PQnvjGN91Mg', \n","                      'HdpRxGjtCo0', 'BI-old7YI4I', 'kmFOBoy2MZ8', \n","                      'bGcvv3683Os', 'JgxkilF5XUM', 's6BQSgidbmc', \n","                      '6VBCxWcAPXw', '2zaIy1TARPE', '3y3MmmfZmP8', \n","                      'xe4Kkbq4An8', 'X4C5fbcYSNg', 'U09K0bQT5PE', \n","                      'X8bBP_cLrl0', 'oyKnBTIoC5E', 'EVicgFd25D4',\n","                      'Ox6pqjQiuJ0', 'fwCl9Ce7MDM', 'aPuDNDZZ6-U', \n","                      '_9MKYKR8lFA', 'vOpH3xnzFJE', 'bq220dgUb0I', \n","                      'lLTdBJsU8N8', 'qXZdRDoGSHo', 'I7yCAmLEDdo', \n","                      'Gogn3p8aDEs', 'TYB8dvCNCQc', 'g_m5VRiKy_E', \n","                      'Gcnf5BdLXxw', '1bJKAu11Ni4', 'OYAgcS31-p0', \n","                      'PPqI-Sk7vsw', 'YWKWkuJwHj4', 'JVhJcXBTl3Y', \n","                      'wfAoq89LNRQ', 'ZSNxaWkuoRo', '4cvZ9NWgsws', \n","                      '-n9uz_cOjT8', '17i2kyEgjWE', '5nE3UO1kqv0',\n","                      'JmF-OOuOxKg', 'WREUb8T4r8o', 'Li7_yFiNaIA', \n","                      'FxrAe5N1xu0', 'CIf6VJH4dZk', 'W77xm6f2sJI',\n","                      '8VzSqYooxmw', 'c7OeeGcMFMc', 'xsMAY4_ICdM', \n","                      'N6wq2eHOZYU', '5VfesP3p0xc', 'X_m1mPtYzTk', \n","                      'H7Uyfqi_TE8', 'UkAVtEoSnoE', 'XQXF3PnSROk',\n","                      'HPTNbPgB5eg', 'JaimO7nvzzQ', '68bu0AeCHm8', \n","                      'QodPNv_XIow', 'j9SdeW5UqTY']"],"id":"extraordinary-stranger","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"outstanding-increase"},"source":["# creating a list of videos to get the comments from \n","completed_videos = [ ]\n","# starting a dataframe for us to add additional comments onto \n","df = pd.DataFrame()\n","\n","#iterating over our list of videos to get their comments and add them to our starting dataframe \n","for item in completed_videos:\n","    # checking if video is in our completed list, easier than remembering or checking visually\n","    if item not in video_list:\n","        get_comments_from_video(video_id=item)\n","        video_list.append(item)"],"id":"outstanding-increase","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"earned-workshop"},"source":["df.to_csv('4.7.yt.csv')"],"id":"earned-workshop","execution_count":null,"outputs":[]}]}