{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "juvenile-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import twython\n",
    "import json\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "peaceful-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "economic-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rocky-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.max_rows', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "monetary-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(df):\n",
    "    \"\"\"\n",
    "    pass in dataframe and returns dataframe with index reset\n",
    "    just wanted to make it a bit of a quicker type since I'm using it so much \n",
    "    \"\"\"\n",
    "    return df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def remove_tweets(df, column, list_to_remove):\n",
    "    \"\"\"\n",
    "    Pass in your dataframe, a column & a list of words to search for and remove.\n",
    "    Does not change capitalization, but will remove if word is within another word.\n",
    "    Returns dataframe with words removed. \n",
    "    \"\"\"\n",
    "    for item in list_to_remove:\n",
    "        reset(df)\n",
    "        for i in range(len(df)):\n",
    "            if type(df.at[i, column]) == str:\n",
    "                col = df.at[i, column]\n",
    "                if item in col:\n",
    "                    df.drop(index=i, inplace=True)\n",
    "    return df \n",
    "\n",
    "def remove_phrase(df, column, dict_to_remove):\n",
    "    \"\"\"\n",
    "    Pass in your dataframe, a column & a dictionary with lowercase key value pairs of words to search for and remove.\n",
    "    Will change capitalization & remove if words are within other words.\n",
    "    Returns dataframe with words removed. \n",
    "    \"\"\"\n",
    "    for key in dict_to_remove.keys():\n",
    "        reset(df)\n",
    "        value = dict_to_remove[key].lower()\n",
    "        key = key.lower()\n",
    "        for i in range(len(df)):\n",
    "            if type(df.at[i, column]) == str:\n",
    "                if key in df.at[i, column] and value in df.at[i, column]:\n",
    "                    df.drop(index=i, inplace=True)\n",
    "    return df \n",
    "\n",
    "def clean_string(string):\n",
    "    for symbol in \"'‚Äô\":\n",
    "        string = string.replace(symbol, '')\n",
    "    for symbol in \"`@#();-=+~:,.?!''\\n/_\\\\\":\n",
    "        string = string.replace(symbol, ' ').lower()\n",
    "    string = string.replace('&', 'and')\n",
    "    return string\n",
    "\n",
    "def count_vectorize(text):\n",
    "    unique_words = set(text)\n",
    "    word_dict = {i:0 for i in unique_words}\n",
    "    \n",
    "    for word in text:\n",
    "        word_dict[word] += 1\n",
    "    \n",
    "    return word_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "korean-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/3.26.twitter')\n",
    "df = df.append(pd.read_csv('data/tweets_matt.csv', low_memory=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "informal-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['user', 'user_id', 'text', 'favorite_count', 'symbols',\n",
    "       'retweet_count', 'mentions', 'hashtag', 'post_id']]\n",
    "reset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "motivated-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since every time we loop through it starts a new index, there are now multiple entries for each index, we need to fix this\n",
    "# and the easiest way is to just reset the index once it finishes running \n",
    "df = df.drop_duplicates(subset='post_id')\n",
    "reset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rational-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to add our strings for our hashtags and mentions together we had to make the entire column a string, sinlucding empty cells\n",
    "# here we go through and replace empty cells with nan values so pandas will read them as being empty instead of a string\n",
    "for i in range(len(df)):\n",
    "    if df.at[i, 'mentions'] == ' ':\n",
    "        df.at[i, 'mentions'] = np.nan\n",
    "    else: \n",
    "        pass \n",
    "    if df.at[i, 'hashtag'] == ' ':\n",
    "        df.at[i, 'hashtag'] = np.nan\n",
    "    else: \n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "engaged-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['favorite_count'] = df['favorite_count'].astype(float).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "developed-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "legendary-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='favorite_count', ascending=False).drop_duplicates(subset='text', keep='first')\n",
    "reset(df)\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pointed-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df.at[i, 'text'] = df.at[i, 'text'].replace('RT', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hearing-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = ['WandaVision', 'findyourthing', 'gangsters', 'RBandME', 'music', 'homework', 'Termpaper', 'newmusic', \n",
    "            'nowplaying', 'fridaylivestream',  'BTSpace', 'CRAVITY',  'Bitcoin', 'StanWorld', 'BecomeOneForIZONE', 'ovni', \n",
    "            'Colchester', 'NowPlaying', 'IZONE_PERMANENT', 'Onlineclass', 'WorldBookDay', 'SatyamevaJayate2', 'izone_permanent',\n",
    "            'SnyderCut', 'ÎßàÎßàÎ¨¥', 'MAMAMOO',  'EVA71', 'ÌïòÍ≤å', 'OurParallelUniverseContinues', 'ÿ¨€åŸà_ÿ™Ÿà_ÿπ€åÿ≥€åŸ∞_⁄©€å_ÿ∑ÿ±ÿ≠', 'dogecoin', \n",
    "            'ShowtimeBetAngMalupet', 'DidYouKnow', 'VoteHarryStyles', 'AMNùóòùó¶ùóúùóî',  'GRAMMYs', 'etsy', 'MyanmarMilitaryTerrorists',\n",
    "            'Poshmark', 'StPatricksDay', 'MindBreeze', 'ad', 'ArtOfTheBlue', '‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ‡§≠‡§æ‡§à_‡§∏‡•Å‡§®‡•ã‡§Ö‡§≤‡•ç‡§≤‡§æ‡§π‡§ï‡•Ä‡§∏‡§ö‡•ç‡§ö‡§æ‡§à', 'Essaydue', 'BigData', \n",
    "            'Aylesbury',  'PiDay', 'Harpenden',  'SoundCloud', 'Dogecoin', 'doge',  'ÿπŸÖÿ±ÿßŸÜ_ŸÜ€åÿßÿ≤€å_⁄ØŸπÿ±_⁄©ÿß_⁄©€å⁄ëÿß', 'ZackSnydersJusticeLeague',\n",
    "            'NFT',  'ifttt', 'Shopee33Comeback', 'ÏõêÏñ¥Ïä§',  'Ïù¥ÎèÑ', 'ÏÜîÎùº_ÎπàÏÑºÏ°∞ost_Adrenaline', 'HadiahLightstickDariShopee', \n",
    "            '3Ïõî24Ïùº_Ï∞¨Ïó¥_ÎçîÎ∞ïÏä§_Í∞úÎ¥â',  'ÏóëÏÜå',  'ÏàòÌò∏',  'ÎîîÏò§', 'ÏãúÏö∞ÎØº', 'ÏàòÌò∏', 'BanglaChaayeBJPModel','CHANYEOL','WeLoveYouBaekhyun',\n",
    "            'OnXiuweetTimeAtHome', 'Ï∞¨Ïó¥', 'NSFW', 'nsfw', '‡§Æ‡§π‡§∞‡•ç‡§∑‡§ø‡§¶‡§Ø‡§æ‡§®‡§Ç‡§¶_‡§ï‡§æ_‡§Ö‡§ú‡•ç‡§û‡§æ‡§®','DollWithBaekhyun',  'BAEKHYUN', 'XIUMIN', 'BCU_RYS21', \n",
    "            'OprahMeghanHarry', 'AuspiXius', 'SUHO', 'DollWithBBHxKDY', 'iCANimagine', 'thewildsspace', 'XiuweetTimeWithYou', 'DYK']\n",
    "\n",
    "users = ['artemis_twt']\n",
    "\n",
    "text = ['esa_celebnews', 'superstraight', 'seekthetruth', 'izone', 'tarotbybronx', 'cryptoart', 'nsfw', 'meme king', 'minecraft', \n",
    "        'artemis and luna', 'brasileiro', 'myanmar coup', 'baekhyun', 'doyoung', 'band', 'kpop', 'cuddles', 'bruno mars', 'space jam']\n",
    "\n",
    "dict_to_remove = {'bruno': 'mars', 'space':'jam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "useful-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_lists = {'user' : users, 'hashtag' : hashtags, 'text': text}\n",
    "\n",
    "for key in dict_of_lists:\n",
    "    df = remove_tweets(df, key, dict_of_lists[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unreasonably computationally expensive? if not nessecary, do not run again\n",
    "# df = remove_phrase(df, 'text', dict_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "light-compiler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Space', 'EXO', 'Area51', 'hindi', 'luna', 'Universe', 'SmartNews', 'MarsDay21', 'Perseverance', 'SpacehotelsTwitterDisco', 'IWD2021', 'Quantum', 'NASAPerseverance', '67P', 'Russia', 'Astrologer', 'KAI', 'moon', 'USA', 'Science', 'Hubble', 'Myanmar', 'Astrophotography', 'CountdownToMars', 'Tigray', 'AskNASA', 'ISS_overLeHaillan', 'SLSgang', 'OTD', 'ASTRO', 'ElonMusk', 'astrophotography', 'job', 'Mars', 'nft', 'FridayLivestream', 'Grammys', 'perseverancerover', 'alienuniverse', 'Mars2020', 'Aliens', 'WomensHistoryMonth', 'Artemis', 'SpaceX', 'internationalwomensday2021', 'Statistics', 'SN11', 'WomensDay', 'Sentinel', 'Starlink', 'OVNI', 'ClimateChange', 'Chicago', 'science', 'NEWS', 'archaeology', 'OnThisDay', 'Marriage', 'STEM', 'StarTrek', 'SLS', 'Wallpaper', 'SpaceHour', 'Nursing', 'onlineclasses', 'news', 'space', 'AstroNomoLogy', 'Cosmology', 'Perseverancerover', 'BREAKING', 'climatechange', 'Futurism', 'APOD', 'Ethiopian', 'InternationalWomensDay', 'exoplanet', 'VirginGalactic', 'EXOLEAVINGSM', 'PerseveranceRover', 'Cosmos', 'Falcon9', 'SolarAdrenalineOST', 'EU', 'Aerospace', 'moonshot', 'astrology', 'InternationalSpaceStation', 'GalacticFederation', 'aerospace', 'MissionAlpha', 'Moon_awards', 'UFO', 'RT', 'GreenRun', 'Mars2021', 'SN10', 'ICYMI', 'Hubble30', 'crypto', 'Louisville', 'think', '10400DaysWithCHEN', 'mars2021', 'StormHour', 'SpacePicture', 'ISS', 'AI', 'ISSoverDaisenTottoriJP', 'MarsPerseverance', 'BritishScienceWeek', 'Nasa', 'perseverance', 'BTC', 'NASA_App', 'Venus', 'COVID19', 'SPACE', 'Eritrean', 'Tesla', 'F1', 'LPSC2021', 'Technology', 'Calculus', 'Astronomy', 'dearMoonCrew', 'bitcoin', 'EXOFREEDOM', 'KeepLookingUp'}\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "empty_list = []\n",
    "\n",
    "for item in df.hashtag.value_counts().to_frame().reset_index()['index']:\n",
    "    if count < 250:\n",
    "        hashtags = str(item).split()\n",
    "        for item in hashtags:\n",
    "            empty_list.append(item)\n",
    "            count = count + 1\n",
    "\n",
    "items = ['jaxa', 'esa', 'curiosityrover', 'areospace', 'internationalspacestation', 'JAXA', 'astronomy',\n",
    "         'oppertunityrover', 'virgingalactic', 'universe', 'sls', 'Starship', 'climate', 'starship', 'virginorbit', \n",
    "         'nasa', 'cosmos', 'mars', 'falconheavy', 'NASA', 'futurism', 'starliner', 'iss', 'spacex', 'falcon9', \n",
    "         'nasa_app', 'roscosmos', 'Roscosmos', 'blueorigin', 'ESA', 'spacetravel', 'artemis', 'marswebcam', 'starlink']\n",
    "\n",
    "fresh_hashtags = []\n",
    "\n",
    "for item in empty_list:\n",
    "    if item not in items:\n",
    "        fresh_hashtags.append(item)\n",
    "        \n",
    "print(set(fresh_hashtags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "civic-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset(df)\n",
    "for i in range(len(df)):\n",
    "    df.at[i, 'text'] = clean_string(str(df.at[i, 'text']))\n",
    "    encoded_string = df.at[i, 'text'].encode(\"ascii\", \"ignore\")\n",
    "    df.at[i, 'text'] = encoded_string.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "spread-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('clean_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "stupid-compromise",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    if type(df.at[i, 'hashtag']) == float:\n",
    "        continue\n",
    "    elif df.at[i, 'hashtag'] in df.at[i, 'text']:\n",
    "        continue\n",
    "    else:\n",
    "        df.at[i, 'text'] = df.at[i, 'text'] + ' ' + df.at[i, 'hashtag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "compliant-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset(df)\n",
    "one_big_list = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    for word in word_tokenize(str(df.at[i, 'text'])):\n",
    "        for symbol in \"'[],\":\n",
    "            word = word.replace(symbol, \"\")\n",
    "        if word != '':\n",
    "            if word.startswith('//') != True:\n",
    "                if word.startswith('http') != True:\n",
    "                    one_big_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "continuing-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = count_vectorize(one_big_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "present-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"data/vect_twts.csv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "writer = csv.writer(a_file)\n",
    "for key, value in vectorized.items():\n",
    "    writer.writerow([key, value])\n",
    "\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ongoing-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('data/vect_twts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "experienced-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.sort_values('1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bridal-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_df = pd.DataFrame(columns=['word', 'count'], index=range(1))\n",
    "add_df.at[0, 'word'] = new_df.columns[0]\n",
    "add_df.at[0, 'count'] = new_df.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "serial-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.rename(columns={new_df.columns[0]:'word', new_df.columns[1]:'count'})\n",
    "new_df = new_df.append(add_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "becoming-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['count'] = new_df['count'].astype(int)\n",
    "new_df = new_df.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "listed-austin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Updating word 1 of 730472, 0.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'Series' objects are mutable, thus they cannot be hashed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-68918a965940>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m             + f\"{(i + 1) * 10000 // (end_point) / 100}%\")\n\u001b[0;32m     23\u001b[0m     \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'word'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mnew_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1668\u001b[1;33m         raise TypeError(\n\u001b[0m\u001b[0;32m   1669\u001b[0m             \u001b[1;34mf\"{repr(type(self).__name__)} objects are mutable, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1670\u001b[0m             \u001b[1;34mf\"thus they cannot be hashed\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Series' objects are mutable, thus they cannot be hashed"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "word_freq = {}\n",
    "end_point = len(df)\n",
    "for i in range(end_point):\n",
    "    clear_output(wait=True)\n",
    "    display(f\"Tweet {i + 1} of {end_point}, \"\n",
    "            + f\"{(i + 1) * 10000 // (end_point) / 100}%\")\n",
    "    text = df.at[i, 'text'].split(\" \")\n",
    "    rt = df.at[i, 'retweet_count'] + 1\n",
    "    for word in text:\n",
    "        word = word.strip()\n",
    "        try:\n",
    "            word_freq[word] += rt\n",
    "        except:\n",
    "            word_freq[word] = rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "inclusive-leadership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Updating word 730472 of 730472, 100.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset(new_df)\n",
    "new_df.to_csv('data/word_count_prior_to_fix.csv')\n",
    "\n",
    "end_point = len(new_df)\n",
    "for i in range(end_point):\n",
    "    clear_output(wait=True)\n",
    "    display(f\"Updating word { i + 1 } of {end_point}, \"\n",
    "            + f\"{(i + 1) * 10000 // (end_point) / 100}%\")\n",
    "    word = new_df.at[i, 'word']\n",
    "    try:\n",
    "        new_df.at[i, 'count'] = word_freq[word]\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "destroyed-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('data/tw_text_counts_incl_rts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "elementary-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['user_id', 'text', 'favorite_count','retweet_count', 'mentions',  'post_id']]\n",
    "df = df.rename(columns={'retweet_count':'repost_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "signed-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/cleaned_tweets.3.30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "brilliant-silver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>repost_count</th>\n",
       "      <th>mentions</th>\n",
       "      <th>post_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.474325e+07</td>\n",
       "      <td>deployment of 60 starlink satellites confirmed...</td>\n",
       "      <td>97534.0</td>\n",
       "      <td>9272.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.367407e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.419640e+07</td>\n",
       "      <td>sn11 almost ready to fly https   t co fxmjjzoikk</td>\n",
       "      <td>60997.0</td>\n",
       "      <td>4389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.371995e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.232783e+18</td>\n",
       "      <td>ive continued driving to scout a spot where il...</td>\n",
       "      <td>56739.0</td>\n",
       "      <td>5605.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.369068e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.860371e+08</td>\n",
       "      <td>hardest jobs in the universe   1  brain surgeo...</td>\n",
       "      <td>44289.0</td>\n",
       "      <td>3939.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.368354e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.167257e+09</td>\n",
       "      <td>honestly  if i hadnt seen this with my own eye...</td>\n",
       "      <td>40107.0</td>\n",
       "      <td>5219.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.371988e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584097</th>\n",
       "      <td>1.774255e+08</td>\n",
       "      <td>starwatch  young moon moves between aldebaran ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.371613e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584098</th>\n",
       "      <td>1.313863e+18</td>\n",
       "      <td>rt  ielielsepulchro  listen to the snap  crack...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>931571805193633794</td>\n",
       "      <td>1.371613e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584099</th>\n",
       "      <td>1.272612e+18</td>\n",
       "      <td>tphlat i have a big david in a secret gian mu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2961319990</td>\n",
       "      <td>1.371613e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584100</th>\n",
       "      <td>1.034832e+18</td>\n",
       "      <td>rt  profmusgrave  again  space is good  but it...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>386513</td>\n",
       "      <td>1.371614e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584101</th>\n",
       "      <td>1.917632e+07</td>\n",
       "      <td>friendsofkrispy when will sydney get the coo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22693855</td>\n",
       "      <td>1.371614e+18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>584102 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id                                               text  \\\n",
       "0       3.474325e+07  deployment of 60 starlink satellites confirmed...   \n",
       "1       4.419640e+07   sn11 almost ready to fly https   t co fxmjjzoikk   \n",
       "2       1.232783e+18  ive continued driving to scout a spot where il...   \n",
       "3       4.860371e+08  hardest jobs in the universe   1  brain surgeo...   \n",
       "4       3.167257e+09  honestly  if i hadnt seen this with my own eye...   \n",
       "...              ...                                                ...   \n",
       "584097  1.774255e+08  starwatch  young moon moves between aldebaran ...   \n",
       "584098  1.313863e+18  rt  ielielsepulchro  listen to the snap  crack...   \n",
       "584099  1.272612e+18   tphlat i have a big david in a secret gian mu...   \n",
       "584100  1.034832e+18  rt  profmusgrave  again  space is good  but it...   \n",
       "584101  1.917632e+07    friendsofkrispy when will sydney get the coo...   \n",
       "\n",
       "        favorite_count  repost_count              mentions       post_id  \n",
       "0              97534.0        9272.0                   NaN  1.367407e+18  \n",
       "1              60997.0        4389.0                   NaN  1.371995e+18  \n",
       "2              56739.0        5605.0                   NaN  1.369068e+18  \n",
       "3              44289.0        3939.0                   NaN  1.368354e+18  \n",
       "4              40107.0        5219.0                   NaN  1.371988e+18  \n",
       "...                ...           ...                   ...           ...  \n",
       "584097             0.0           0.0                   NaN  1.371613e+18  \n",
       "584098             0.0           2.0    931571805193633794  1.371613e+18  \n",
       "584099             0.0           0.0            2961319990  1.371613e+18  \n",
       "584100             0.0           1.0                386513  1.371614e+18  \n",
       "584101             0.0           0.0              22693855  1.371614e+18  \n",
       "\n",
       "[584102 rows x 6 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-upset",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
