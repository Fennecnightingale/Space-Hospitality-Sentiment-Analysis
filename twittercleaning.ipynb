{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brazilian-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import twython\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thermal-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "previous-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "acceptable-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(df):\n",
    "    \"\"\"\n",
    "    pass in dataframe and returns dataframe with index reset\n",
    "    just wanted to make it a bit of a quicker type since I'm using it so much \n",
    "    \"\"\"\n",
    "    return df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def remove_tweets(df, column, list_to_remove):\n",
    "    \"\"\"\n",
    "    Pass in your dataframe, a column & a list of words to search for and remove.\n",
    "    Does not change capitalization, but will remove if word is within another word.\n",
    "    Returns dataframe with words removed. \n",
    "    \"\"\"\n",
    "    for item in list_to_remove:\n",
    "        reset(df)\n",
    "        for i in range(len(df)):\n",
    "            if type(df.at[i, column]) == str:\n",
    "                if item in df.at[i, column]:\n",
    "                    df.drop(index=i, inplace=True)\n",
    "    return df \n",
    "\n",
    "def remove_phrase(df, column, dict_to_remove):\n",
    "    \"\"\"\n",
    "    Pass in your dataframe, a column & a dictionary with lowercase key value pairs of words to search for and remove.\n",
    "    Will change capitalization & remove if words are within other words.\n",
    "    Returns dataframe with words removed. \n",
    "    \"\"\"\n",
    "    for key in dict_to_remove.keys():\n",
    "        reset(df)\n",
    "        value = dict_to_remove[key].lower()\n",
    "        key = key.lower()\n",
    "        for i in range(len(df)):\n",
    "            if type(df.at[i, column]) == str:\n",
    "                if key in df.at[i, column] and value in df.at[i, column]:\n",
    "                    df.drop(index=i, inplace=True)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "direct-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tw_319_nd')\n",
    "df = df.append(pd.read_csv('beeep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "entitled-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0', 'level_0', 'index', 'Unnamed: 0.1', 'Unnamed: 1'], 1)\n",
    "reset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "innocent-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since every time we loop through it starts a new index, there are now multiple entries for each index, we need to fix this\n",
    "# and the easiest way is to just reset the index once it finishes running \n",
    "df = df.drop_duplicates(subset='post_id')\n",
    "reset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "excited-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to add our strings for our hashtags and mentions together we had to make the entire column a string, sinlucding empty cells\n",
    "# here we go through and replace empty cells with nan values so pandas will read them as being empty instead of a string\n",
    "for i in range(len(df)):\n",
    "    if df.at[i, 'mentions'] == ' ':\n",
    "        df.at[i, 'mentions'] = np.nan\n",
    "    else: \n",
    "        pass \n",
    "    if df.at[i, 'hashtag'] == ' ':\n",
    "        df.at[i, 'hashtag'] = np.nan\n",
    "    else: \n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "informative-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='favorite_count', ascending=False).drop_duplicates(subset='text', keep='first')\n",
    "reset(df)\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "distinguished-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df.at[i, 'text'] = df.at[i, 'text'].replace('RT', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "stylish-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = ['WandaVision', 'findyourthing', 'gangsters', 'RBandME', 'music', 'homework', 'Termpaper', 'esa', 'newmusic', \n",
    "            'nowplaying', 'fridaylivestream',  'BTSpace', 'CRAVITY',  'Bitcoin', 'StanWorld', 'BecomeOneForIZONE', 'ovni', \n",
    "            'Colchester', 'NowPlaying', 'IZONE_PERMANENT', 'Onlineclass', 'WorldBookDay', 'SatyamevaJayate2', 'izone_permanent',\n",
    "            'SnyderCut', 'ÎßàÎßàÎ¨¥', 'MAMAMOO',  'EVA71', 'ÌïòÍ≤å', 'OurParallelUniverseContinues', 'ÿ¨€åŸà_ÿ™Ÿà_ÿπ€åÿ≥€åŸ∞_⁄©€å_ÿ∑ÿ±ÿ≠', 'dogecoin', \n",
    "            'ShowtimeBetAngMalupet', 'DidYouKnow', 'VoteHarryStyles', 'AMNùóòùó¶ùóúùóî',  'GRAMMYs', 'etsy', 'MyanmarMilitaryTerrorists',\n",
    "            'Poshmark', 'StPatricksDay', 'MindBreeze', 'ad', 'ArtOfTheBlue', '‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ‡§≠‡§æ‡§à_‡§∏‡•Å‡§®‡•ã‡§Ö‡§≤‡•ç‡§≤‡§æ‡§π‡§ï‡•Ä‡§∏‡§ö‡•ç‡§ö‡§æ‡§à', 'Essaydue', 'BigData', \n",
    "            'Aylesbury',  'PiDay', 'Harpenden',  'SoundCloud', 'Dogecoin', 'doge',  'ÿπŸÖÿ±ÿßŸÜ_ŸÜ€åÿßÿ≤€å_⁄ØŸπÿ±_⁄©ÿß_⁄©€å⁄ëÿß', 'ZackSnydersJusticeLeague',\n",
    "            'NFT',  'ifttt']\n",
    "\n",
    "users = ['artemis_twt']\n",
    "\n",
    "text = ['esa_celebnews']\n",
    "\n",
    "dict_to_remove = {'bruno': 'mars', 'space':'jam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "indirect-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_lists = {'user' : users, 'hashtag' : hashtags, 'text': text}\n",
    "\n",
    "for key in dict_of_lists:\n",
    "    df = remove_tweets(df, key, dict_of_lists[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "alpine-spyware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358422"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = remove_phrase(df, 'text', dict_to_remove)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "hollywood-tolerance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StarWalk', 'ElonMusk', 'InternationalWomensDay', 'BREAKING', 'ISSoverDaisenTottoriJP', 'Astronomy', 'FridayLivestream', 'SPACE', 'think', 'dearMoonCrew', 'dearmooncrew', 'EU', 'luna', 'energystorage', 'ParkerSolarProbe', 'Perseverance', 'Nasa', 'Starlink', 'space', 'industry', 'Falcon9', 'SN10', 'BritishScienceWeek', 'STEM', 'Challenger', 'USA', 'ISS', 'AI', 'TournamentEarth', 'science', 'AskNASA', 'job', 'WomenInSTEM', 'DYK', 'Cosmos', 'SmartNews', 'Webb', 'SpaceXFleet', 'Wallpaper', 'Technology', 'Venus', 'copper', 'Quantum', 'News', 'rehearsals', 'meeting', 'skywatching', 'LPSC2021', 'StarTrek', 'astrophotography', 'KeepLookingUp', 'spaceflight', 'StormHour', 'OTD', 'Cosmology', 'SpaceX', 'Myanmar', 'MissionAlpha', 'Futurism', 'Aerospace', 'denver', 'IWD2021', 'apod', 'F1', 'SpaceHour', 'Marriage', 'hindi', 'VirginGalactic', 'Sentinel', 'NASA_App', '67P', 'Spacex', 'PerseveranceRover', 'YoutubeIndia', 'WomensHistoryMonth', 'OnThisDay', 'InternationalSpaceStation', 'COVID19', 'Russia', 'Astrologer', 'ISS_overLeHaillan', 'Hubble30', 'NSSC2021', 'news', 'moonshot', 'Artemis', 'ClimateChange', 'AstroNomoLogy', 'APOD', 'Universe', 'Sentinel2', 'SN11', 'Space', 'Chicago', 'aerospace', 'FalconHeavy', 'astrology', 'moon', 'Australia', 'Science', 'ICYMI', 'internationalwomensday2021', 'NEWS', 'Moon_awards', 'Louisville', 'film', 'climatechange', 'GreenRun', 'Tesla', 'RT', 'exploration', 'astronomyapp', 'ASTRO', 'SLS', 'SLSgang'}\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "empty_list = []\n",
    "\n",
    "for item in df.hashtag.value_counts().to_frame().reset_index()['index']:\n",
    "    if count < 250:\n",
    "        hashtags = str(item).split()\n",
    "        for item in hashtags:\n",
    "            empty_list.append(item)\n",
    "            count = count + 1\n",
    "\n",
    "items = ['jaxa', 'esa', 'curiosityrover', 'areospace', 'internationalspacestation', 'JAXA', 'astronomy',\n",
    "         'oppertunityrover', 'virgingalactic', 'universe', 'sls', 'Starship', 'climate', 'starship', 'virginorbit', \n",
    "         'nasa', 'cosmos', 'mars', 'falconheavy', 'NASA', 'futurism', 'starliner', 'iss', 'spacex', 'falcon9', \n",
    "         'nasa_app', 'roscosmos', 'Roscosmos', 'blueorigin', 'ESA', 'spacetravel', 'artemis', 'marswebcam', 'starlink']\n",
    "\n",
    "fresh_hashtags = []\n",
    "\n",
    "for item in empty_list:\n",
    "    if item not in items:\n",
    "        fresh_hashtags.append(item)\n",
    "        \n",
    "print(set(fresh_hashtags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adequate-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(text):\n",
    "#     tokenized = word_tokenize(text)\n",
    "#     return tokenized\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     df.at[i, 'text'] = tokenize(df.at[i, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "funny-birth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164467\n",
      "202489\n",
      "226925\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('yt')\n",
    "print(len(df))\n",
    "df = df.append(pd.read_csv('yt1'))\n",
    "print(len(df))\n",
    "df = df.append(pd.read_csv('yt2'))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "proper-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('yt0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "stretch-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ''\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     word_list = df.at[i, 'text']\n",
    "#     for word in word_list:\n",
    "#         all_words = all_words + '' + word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "urban-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_ = all_words.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ranging-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_ = {'item': [], 'count':[]}\n",
    "# for item in list_:\n",
    "#     dict_['item'].append(item)\n",
    "#     dict_['count'].append(list_.count(item))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
