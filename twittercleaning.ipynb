{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "metropolitan-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import twython\n",
    "import json\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "careful-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "medium-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "framed-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.max_rows', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opening-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(df):\n",
    "    \"\"\"\n",
    "    pass in dataframe and returns dataframe with index reset\n",
    "    just wanted to make it a bit of a quicker type since I'm using it so much \n",
    "    \"\"\"\n",
    "    return df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def remove_tweets(df, column, list_to_remove):\n",
    "    \"\"\"\n",
    "    Pass in your dataframe, a column & a list of words to search for and remove.\n",
    "    Does not change capitalization, but will remove if word is within another word.\n",
    "    Returns dataframe with words removed. \n",
    "    \"\"\"\n",
    "    for item in list_to_remove:\n",
    "        reset(df)\n",
    "        for i in range(len(df)):\n",
    "            if type(df.at[i, column]) == str:\n",
    "                col = df.at[i, column]\n",
    "                if item in col or item in col.lower:\n",
    "                    df.drop(index=i, inplace=True)\n",
    "    return df \n",
    "\n",
    "def remove_phrase(df, column, dict_to_remove):\n",
    "    \"\"\"\n",
    "    Pass in your dataframe, a column & a dictionary with lowercase key value pairs of words to search for and remove.\n",
    "    Will change capitalization & remove if words are within other words.\n",
    "    Returns dataframe with words removed. \n",
    "    \"\"\"\n",
    "    for key in dict_to_remove.keys():\n",
    "        reset(df)\n",
    "        value = dict_to_remove[key].lower()\n",
    "        key = key.lower()\n",
    "        for i in range(len(df)):\n",
    "            if type(df.at[i, column]) == str:\n",
    "                if key in df.at[i, column] and value in df.at[i, column]:\n",
    "                    df.drop(index=i, inplace=True)\n",
    "    return df \n",
    "\n",
    "def clean_string(string):\n",
    "    for symbol in \"'‚Äô\":\n",
    "        string = string.replace(symbol, '')\n",
    "    for symbol in \"`@#();-=+~:,.?!''\\n/_\\\\\":\n",
    "        string = string.replace(symbol, ' ').lower()\n",
    "    string = string.replace('&', 'and')\n",
    "    return string\n",
    "\n",
    "def count_vectorize(text):\n",
    "    unique_words = set(text)\n",
    "    word_dict = {i:0 for i in unique_words}\n",
    "    \n",
    "    for word in text:\n",
    "        word_dict[word] += 1\n",
    "    \n",
    "    return word_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "respective-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/3.26.twitter')\n",
    "df = df.append(pd.read_csv('data/tweets_matt.csv', low_memory=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recovered-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['user', 'user_id', 'text', 'favorite_count', 'symbols',\n",
    "       'retweet_count', 'mentions', 'hashtag', 'post_id']]\n",
    "reset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "impossible-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since every time we loop through it starts a new index, there are now multiple entries for each index, we need to fix this\n",
    "# and the easiest way is to just reset the index once it finishes running \n",
    "df = df.drop_duplicates(subset='post_id')\n",
    "reset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dried-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to add our strings for our hashtags and mentions together we had to make the entire column a string, sinlucding empty cells\n",
    "# here we go through and replace empty cells with nan values so pandas will read them as being empty instead of a string\n",
    "for i in range(len(df)):\n",
    "    if df.at[i, 'mentions'] == ' ':\n",
    "        df.at[i, 'mentions'] = np.nan\n",
    "    else: \n",
    "        pass \n",
    "    if df.at[i, 'hashtag'] == ' ':\n",
    "        df.at[i, 'hashtag'] = np.nan\n",
    "    else: \n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "arabic-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['favorite_count'] = df['favorite_count'].astype(float).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "competitive-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tracked-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='favorite_count', ascending=False).drop_duplicates(subset='text', keep='first')\n",
    "reset(df)\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "appreciated-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df.at[i, 'text'] = df.at[i, 'text'].replace('RT', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "magnetic-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = ['WandaVision', 'findyourthing', 'gangsters', 'RBandME', 'music', 'homework', 'Termpaper', 'newmusic', \n",
    "            'nowplaying', 'fridaylivestream',  'BTSpace', 'CRAVITY',  'Bitcoin', 'StanWorld', 'BecomeOneForIZONE', 'ovni', \n",
    "            'Colchester', 'NowPlaying', 'IZONE_PERMANENT', 'Onlineclass', 'WorldBookDay', 'SatyamevaJayate2', 'izone_permanent',\n",
    "            'SnyderCut', 'ÎßàÎßàÎ¨¥', 'MAMAMOO',  'EVA71', 'ÌïòÍ≤å', 'OurParallelUniverseContinues', 'ÿ¨€åŸà_ÿ™Ÿà_ÿπ€åÿ≥€åŸ∞_⁄©€å_ÿ∑ÿ±ÿ≠', 'dogecoin', \n",
    "            'ShowtimeBetAngMalupet', 'DidYouKnow', 'VoteHarryStyles', 'AMNùóòùó¶ùóúùóî',  'GRAMMYs', 'etsy', 'MyanmarMilitaryTerrorists',\n",
    "            'Poshmark', 'StPatricksDay', 'MindBreeze', 'ad', 'ArtOfTheBlue', '‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ‡§≠‡§æ‡§à_‡§∏‡•Å‡§®‡•ã‡§Ö‡§≤‡•ç‡§≤‡§æ‡§π‡§ï‡•Ä‡§∏‡§ö‡•ç‡§ö‡§æ‡§à', 'Essaydue', 'BigData', \n",
    "            'Aylesbury',  'PiDay', 'Harpenden',  'SoundCloud', 'Dogecoin', 'doge',  'ÿπŸÖÿ±ÿßŸÜ_ŸÜ€åÿßÿ≤€å_⁄ØŸπÿ±_⁄©ÿß_⁄©€å⁄ëÿß', 'ZackSnydersJusticeLeague',\n",
    "            'NFT',  'ifttt', 'Shopee33Comeback', 'ÏõêÏñ¥Ïä§',  'Ïù¥ÎèÑ', 'ÏÜîÎùº_ÎπàÏÑºÏ°∞ost_Adrenaline', 'HadiahLightstickDariShopee', \n",
    "            '3Ïõî24Ïùº_Ï∞¨Ïó¥_ÎçîÎ∞ïÏä§_Í∞úÎ¥â',  'ÏóëÏÜå',  'ÏàòÌò∏',  'ÎîîÏò§', 'ÏãúÏö∞ÎØº', 'ÏàòÌò∏', 'BanglaChaayeBJPModel','CHANYEOL','WeLoveYouBaekhyun',\n",
    "            'OnXiuweetTimeAtHome', 'Ï∞¨Ïó¥', 'NSFW', 'nsfw', '‡§Æ‡§π‡§∞‡•ç‡§∑‡§ø‡§¶‡§Ø‡§æ‡§®‡§Ç‡§¶_‡§ï‡§æ_‡§Ö‡§ú‡•ç‡§û‡§æ‡§®','DollWithBaekhyun',  'BAEKHYUN', 'XIUMIN', 'BCU_RYS21', \n",
    "            'OprahMeghanHarry', 'AuspiXius', 'SUHO', 'DollWithBBHxKDY', 'iCANimagine', 'thewildsspace', 'XiuweetTimeWithYou', 'DYK']\n",
    "\n",
    "users = ['artemis_twt']\n",
    "\n",
    "text = ['esa_celebnews', 'superstraight', 'seekthetruth', 'izone', 'tarotbybronx', 'cryptoart', 'nsfw', 'meme king', 'minecraft', \n",
    "        'artemis and luna', 'brasileiro', 'myanmar coup', 'baekhyun', 'doyoung', 'band', 'kpop', 'cuddles']\n",
    "\n",
    "dict_to_remove = {'bruno': 'mars', 'space':'jam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "spiritual-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_lists = {'user' : users, 'hashtag' : hashtags, 'text': text}\n",
    "\n",
    "for key in dict_of_lists:\n",
    "    df = remove_tweets(df, key, dict_of_lists[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "coupled-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unreasonably computationally expensive? if not nessecary, do not run again\n",
    "df = remove_phrase(df, 'text', dict_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "handled-selling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aliens', 'IWD2021', 'BTC', 'science', 'SolarAdrenalineOST', 'Ethiopian', 'astrophotography', 'ElonMusk', 'SmartNews', 'WomensDay', 'COVID19', 'Hubble30', 'ISS', 'SpacePicture', 'ISS_overLeHaillan', 'nft', 'APOD', 'bitcoin', 'WomensHistoryMonth', 'Astronomy', 'SpaceX', 'Moon_awards', 'Nasa', 'Louisville', 'Astrophotography', 'Starlink', 'Quantum', 'Hubble', 'KeepLookingUp', 'NEWS', 'RT', 'EXOLEAVINGSM', 'Technology', 'news', '10400DaysWithCHEN', 'internationalwomensday2021', 'Wallpaper', 'SN10', 'AskNASA', 'astrology', 'LPSC2021', 'NASA_App', 'dearMoonCrew', 'job', 'OTD', 'Science', 'Universe', 'ASTRO', 'InternationalWomensDay', 'StarTrek', 'UFO', 'MarsDay21', 'OVNI', 'Tigray', 'InternationalSpaceStation', 'EU', 'exoplanet', 'StormHour', 'onlineclasses', 'SLS', 'Aerospace', 'MarsPerseverance', 'Chicago', 'Venus', 'USA', 'mars2021', 'Nursing', 'perseverance', 'CountdownToMars', 'SPACE', 'AstroNomoLogy', 'hindi', 'VirginGalactic', 'crypto', 'Mars2021', 'Myanmar', 'Statistics', 'Space', 'Mars', 'Astrologer', 'NASAPerseverance', 'EXOFREEDOM', 'Artemis', 'GreenRun', 'aerospace', 'PerseveranceRover', 'KAI', 'moonshot', 'BREAKING', 'F1', 'Cosmology', 'Falcon9', 'STEM', 'MissionAlpha', 'luna', 'space', 'archaeology', 'Sentinel', 'Area51', 'AI', 'climatechange', 'Futurism', 'ISSoverDaisenTottoriJP', 'ICYMI', 'TREASURE', 'think', 'Mars2020', 'FridayLivestream', 'SpaceHour', 'SpacehotelsTwitterDisco', 'Marriage', 'Eritrean', 'perseverancerover', 'SN11', 'Tesla', 'BritishScienceWeek', 'Calculus', 'Perseverancerover', 'SLSgang', 'EXO', '67P', 'ClimateChange', 'Perseverance', 'moon', 'OnThisDay', 'Russia', 'alienuniverse', 'GalacticFederation', 'Cosmos'}\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "empty_list = []\n",
    "\n",
    "for item in df.hashtag.value_counts().to_frame().reset_index()['index']:\n",
    "    if count < 250:\n",
    "        hashtags = str(item).split()\n",
    "        for item in hashtags:\n",
    "            empty_list.append(item)\n",
    "            count = count + 1\n",
    "\n",
    "items = ['jaxa', 'esa', 'curiosityrover', 'areospace', 'internationalspacestation', 'JAXA', 'astronomy',\n",
    "         'oppertunityrover', 'virgingalactic', 'universe', 'sls', 'Starship', 'climate', 'starship', 'virginorbit', \n",
    "         'nasa', 'cosmos', 'mars', 'falconheavy', 'NASA', 'futurism', 'starliner', 'iss', 'spacex', 'falcon9', \n",
    "         'nasa_app', 'roscosmos', 'Roscosmos', 'blueorigin', 'ESA', 'spacetravel', 'artemis', 'marswebcam', 'starlink']\n",
    "\n",
    "fresh_hashtags = []\n",
    "\n",
    "for item in empty_list:\n",
    "    if item not in items:\n",
    "        fresh_hashtags.append(item)\n",
    "        \n",
    "print(set(fresh_hashtags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hairy-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset(df)\n",
    "for i in range(len(df)):\n",
    "    df.at[i, 'text'] = clean_string(str(df.at[i, 'text']))\n",
    "    encoded_string = df.at[i, 'text'].encode(\"ascii\", \"ignore\")\n",
    "    df.at[i, 'text'] = encoded_string.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "precious-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('clean_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sharp-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset(df)\n",
    "one_big_list = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    for word in word_tokenize(str(df.at[i, 'text'])):\n",
    "        for symbol in \"'[],\":\n",
    "            word = word.replace(symbol, \"\")\n",
    "        if word != '':\n",
    "            if word.startswith('//') != True:\n",
    "                if word.startswith('http') != True:\n",
    "                    one_big_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "looking-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = count_vectorize(one_big_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unexpected-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"data/vect_twts.csv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "writer = csv.writer(a_file)\n",
    "for key, value in vectorized.items():\n",
    "    writer.writerow([key, value])\n",
    "\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "gothic-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('data/vect_twts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "armed-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.sort_values('1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "opened-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_df = pd.DataFrame(columns=['word', 'count'], index=range(1))\n",
    "add_df.at[0, 'word'] = new_df.columns[0]\n",
    "add_df.at[0, 'count'] = new_df.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "minimal-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.rename(columns={new_df.columns[0]:'word', new_df.columns[1]:'count'})\n",
    "new_df = new_df.append(add_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "affecting-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['count'] = new_df['count'].astype(int)\n",
    "new_df = new_df.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(new_df['word'])):\n",
    "    spaced_word = f\" {new_df.at[i, 'word']} \"\n",
    "    for j in range(len(df)):\n",
    "        repost_count = df.at[j, 'repost_count']\n",
    "        word_count = len(re.findall(spaced_word, f\" {df.at[j, 'text']} \"))\n",
    "        new_df.at[i, \"count\"] += word_count * repost_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('data/tw_text_counts_incl_rts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    if type(df.at[i, 'hashtag']) == float:\n",
    "        continue\n",
    "    elif df.at[i, 'hashtag'] in df.at[i, 'text']:\n",
    "        continue\n",
    "    else:\n",
    "        df.at[i, 'text'] = df.at[i, 'text'] + ' ' + df.at[i, 'hashtag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['user_id', 'text', 'favorite_count','retweet_count', 'mentions',  'post_id']]\n",
    "df = df.rename(columns={'retweet_count':'repost_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/cleaned_tweets.3.30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-adelaide",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
