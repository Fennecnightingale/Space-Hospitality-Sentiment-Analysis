{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing, Sentiment Analysis, and Recommendation Modeling\n",
    "#### Authors: Fennec Nighintgale, Matthew Lipman\n",
    "\n",
    "In this notebook, we will be walking through using Faiss Kmeans algorithm and Naive Bayes to classify and model sentiment on Tweets and Youtube comments all relating to interstellar travel and space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import scipy\n",
    "from scipy.linalg import norm\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from scipy.sparse import csr_matrix, coo_matrix, hstack, vstack\n",
    "\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import silhouette_samples, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, CategoricalNB\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_an(df):\n",
    "  \"\"\"\n",
    "  Given a pandas dataframe, this will shuffle it & get sentiment \n",
    "  analysis values from TextBlob & VaderSentiment & assign columns inplace\n",
    "  Will return completed dataframe & dataframe columns we'll be using for\n",
    "  modeling as a coo_matrix. \n",
    "                Columns returned: \n",
    "  'polarity' : mean of text sentiment (TextBlob)\n",
    "  'neg': negative sentiment value (VaderSentiment)\n",
    "  'pos': positive sentiment value (VaderSentiment)\n",
    "  'neu': neutral sentiment value (VaderSeniment)\n",
    "  \"\"\"\n",
    "  # shuffling our dataframe so data is no longer sorted\n",
    "  df = df.sample(frac = 1)\n",
    "  df = df.reset_index().rename(columns={'index':'id'})\n",
    "  # getting rid of all unnessecary columns\n",
    "  df = df[['favorite_count', 'repost_count', 'text', 'id']]\n",
    "  # feature generation using polarization \n",
    "  df['polarity'] = df['text'].apply(lambda x: \n",
    "                                    TextBlob(x).sentiment.polarity)\n",
    "  # feature generation using intensity analyzer \n",
    "  analyzer = SentimentIntensityAnalyzer()\n",
    "  df['pol'] = ''\n",
    "  for i in range(len(df)):\n",
    "    sentence = df.at[i, 'text']\n",
    "    df.at[i, 'pol'] = analyzer.polarity_scores(sentence)\n",
    "  for i in range(len(df)):\n",
    "    for h in ['neg', 'neu', 'pos']:\n",
    "      df.at[i, h] = float(df['pol'][i][h])\n",
    "  #turning our other variables into a matrix to combine our features \n",
    "  maxab = MaxAbsScaler()\n",
    "  drop = ['text', 'pol', 'id']\n",
    "  hstack = maxab.fit_transform(coo_matrix(df.drop(labels=drop,  axis=1)))\n",
    "  return df, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_transform(df):\n",
    "  \"\"\"\n",
    "  Pass in a pandas dataframe and this will return a coo_matrix\n",
    "  featuring the TFIDF vectorization of your text, with a maximum\n",
    "  of 100 features of single words and 100 features of bigrams. \n",
    "  \"\"\"\n",
    "  sing = TfidfVectorizer(max_features=100).fit_transform(df['text'])\n",
    "  bi = TfidfVectorizer(ngram_range = (2, 2), \n",
    "                       max_features=100).fit_transform(df['text'])\n",
    "  vect = coo_matrix(np.append(bi.toarray(), sing.toarray(), axis=1))\n",
    "  return vect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_pca(vect, haystack):\n",
    "  \"\"\"\n",
    "  Pass in your coo_matrix of your word vectors & the scaled \n",
    "  features from your dataframe this will perform Principal Component \n",
    "  Analysis in order to keep your most relevant 100 features, calculated\n",
    "   in chunks to avoid RAM issues. It will return a numpy\n",
    "   array containing all of the data for your model. \n",
    "  \"\"\"\n",
    "  svd = TruncatedSVD(n_components=95, n_iter=250000, \n",
    "                     random_state=0, algorithm='arpack').fit(vect)\n",
    "  # splitting it so we never keep it all in RAM at once as an array \n",
    "  splits = []\n",
    "  shape = vect.shape[1]\n",
    "  tbs = vect \n",
    "  # since you cant directly index a matrix the easiest way is train_test\n",
    "  # chose 50000 since my computer can handle it \n",
    "  while shape > 50000:\n",
    "      shape = shape - 50000\n",
    "      # first, run through the whole matrix \n",
    "      split1, split2 = train_test_split(tbs, random_state=0, \n",
    "                                        train_size=shape)\n",
    "      #all of our leftover rows (not in the 30k) are in split2\n",
    "      tbs = split2\n",
    "      # put the chunk of our matrix into a list to iterate over later \n",
    "      splits.append(split1)\n",
    "  # append the remainder (whatever was left < 30k) to the end of our list \n",
    "  splits.append(tbs)\n",
    "  # start a new matrix by fit transforming our first chunk\n",
    "  init = coo_matrix(svd.fit_transform(splits[0]))\n",
    "  # start a loop that will fit transform and stack matricies together\n",
    "  for item in splits[1:]:\n",
    "      # fit transform will return it as a dense array \n",
    "      init = vstack([init, coo_matrix(svd.fit_transform(item))])\n",
    "  # combine our text matrix & our repost/favorite matrix\n",
    "  return np.append(haystack.toarray(), \n",
    "                   init.toarray(), axis=1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect_and_stck(dataframe, first_run, drop_clust):\n",
    "  \"\"\"\n",
    "  Pass in a DataFrame & this function will preprocess and return your\n",
    "  dataframe & all of the columns nessecary to model as an array\n",
    "  The first step is to get sentiment analysis scores, \n",
    "  which is only nessecary the first time you run this, otherwise\n",
    "  they can be turned to False to save time, \n",
    "  sentiment analysis is followed by TDIDF vectorizaton on \n",
    "  single words & bigrams, and lastly it ends with PCA to reduce \n",
    "  our dimensions to a reasonable amount. Make drop_clust True to\n",
    "  return a copy of your dataframe with no cluster column before \n",
    "  rerunning your model on it. \n",
    "  \"\"\"\n",
    "  if first_run == True:\n",
    "    dataframe, haystack = get_sentiment_an(dataframe)\n",
    "  else:\n",
    "    clmn = ['neg', 'neu', 'pos', 'polarity',\n",
    "            'repost_count', 'favorite_count']\n",
    "    haystack = coo_matrix(dataframe[clmn])\n",
    "  vect = word_transform(dataframe)\n",
    "  if drop_clust == True:\n",
    "    dataframe = dataframe.drop('cluster', 1)\n",
    "  return svd_pca(vect, haystack), dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_elbow(haystack, df, display):\n",
    "  \"\"\"\n",
    "  Plots your clusters from k 1-5 and returns the best one as an int,\n",
    "  if display is true it will display the elbow plot, if not it will \n",
    "  just return your value. \n",
    "  \"\"\"\n",
    "  max = 5\n",
    "  K = range(1,max+1)\n",
    "  # start numpy arrays to store results in \n",
    "  inertias = np.zeros(max)\n",
    "  diff = np.zeros(max)\n",
    "  diff2 = np.zeros(max)\n",
    "  diff3 = np.zeros(max)\n",
    "  for k in K:\n",
    "    kmeans, predictions = clusterizer(haystack, df, False)\n",
    "    inertias[k - 1] = kmeans.obj[-1]\n",
    "    # first difference    \n",
    "    if k > 1:\n",
    "        diff[k - 1] = inertias[k - 1] - inertias[k - 2]\n",
    "    # second difference\n",
    "    elif k > 2:\n",
    "        diff2[k - 1] = diff[k - 1] - diff[k - 2]\n",
    "    # third difference\n",
    "    elif k > 3:\n",
    "        diff3[k - 1] = diff2[k - 1] - diff2[k - 2]\n",
    "  # use differences & numpy argmin to determine best cluster\n",
    "  elbow = np.argmin(diff3[3:]) + 3\n",
    "  if display == True: \n",
    "    print(f'Elbow {str(elbow)}')\n",
    "    plt.plot(K, inertias, 'b*-')\n",
    "    plt.plot(K[elbow], inertias[elbow], marker='o', markersize=12,\n",
    "                markeredgewidth=2, markeredgecolor='r')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.xlabel('K')\n",
    "    plt.show()\n",
    "  elbow = int(elbow)\n",
    "  global findimb\n",
    "  findimb += elbow - 1\n",
    "  return elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_samples(dataframe, stack, fit_predict, drop):\n",
    "  \"\"\"\n",
    "  Given your dataframe, feature array/matrix & predictions \n",
    "  Computes Silhouette Scores using sklearn and chunking \n",
    "  the data into sets of 75000 and returns a deep copy of\n",
    "  your dataframe with the silhouette scores in the column \n",
    "  'sil' Scores will not be perfect as this function chunks them, and \n",
    "  therefore cannot get complete pairwise distances  \n",
    "  but without chunking silhouette scores cannot be run on \n",
    "  any computer or service we can find and it will be pretty close.\n",
    "  Use drop = True to calculate the z-score of your silhouette scores\n",
    "  and get rid of the bottom 10%, assuming they're likely to be outliers. \n",
    "  \"\"\"\n",
    "  start = 0 \n",
    "  clust = 75000\n",
    "  for i in range(math.ceil(stack.shape[0]/75000)):\n",
    "    df = dataframe[start:start+clust].reset_index(drop=True).copy()\n",
    "    fp = np.ravel(fit_predict[start:start+clust][0:])\n",
    "    if type(stack) != np.ndarray:\n",
    "      hstk = stack.toarray()[start:start+clust][0:]\n",
    "    else:\n",
    "      hstk = stack[start:start+clust][0:]\n",
    "    df['sil'] = silhouette_samples(hstk, fp)\n",
    "    if i == 0:\n",
    "      head = df\n",
    "    else:\n",
    "      head = head.append(df) \n",
    "    if drop == True:\n",
    "      mean = head['sil'].mean()\n",
    "      std = head['sil'].std()\n",
    "      head.reset_index(drop=True, inplace=True)\n",
    "      for j in range(len(head)):\n",
    "        head.at[j, 'silstd'] = (head.at[j, 'sil'] - mean)/std\n",
    "      head = head.loc[head['silstd'] > -1.28].drop('silstd', 1)\n",
    "    start += 75000\n",
    "  return head.sample(frac=1).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_silhouette(dataframe, stack, fit_predict):\n",
    "  \"\"\"\n",
    "  Pass in your dataframe, your feature array/matrix, \n",
    "  & your predictions, and this will use chunked_samples\n",
    "  to get your score & try to quickly form a matplotlib visual\n",
    "  Scores won't be 100% accurate as you cannot get pairwise distances \n",
    "  with chunking, but they use a sample size of 75,000 so they should\n",
    "  be close, and the decrease in accuracy is well worth the 6-7 hours\n",
    "  in speed it saves\n",
    "  \"\"\"\n",
    "  _dataframe = chunked_samples(dataframe, stack, fit_predict, False)\n",
    "  silhouette_vals = _dataframe['sil'].to_frame().to_numpy()\n",
    "  labels = np.unique(fit_predict)\n",
    "  y_ax_lower, y_ax_upper = 0, 0\n",
    "  yticks = []\n",
    "  for i, c in enumerate(labels):\n",
    "    c_silhouette_vals = silhouette_vals[fit_predict == c]\n",
    "    c_silhouette_vals.sort()\n",
    "    y_ax_upper += len(c_silhouette_vals)\n",
    "    color = cm.jet(float(i) / labels.shape[0])\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, \n",
    "             height=1.0, color=color)\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "    y_ax_lower += len(c_silhouette_vals)\n",
    "  plt.axvline(np.mean(silhouette_vals), color=\"red\", linestyle=\"--\") \n",
    "  plt.yticks(yticks, labels + 1)\n",
    "  plt.ylabel('Cluster')\n",
    "  plt.xlabel('Silhouette Coefficient')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imb_clstr(dataframe):\n",
    "  \"\"\"\n",
    "  Pass in your dataframe and get the cluster with the most values in it \n",
    "  returned as an interger \n",
    "  \"\"\"\n",
    "  clstr = dataframe['cluster'].value_counts()\n",
    "  clstr = int(clstr.to_frame().reset_index()['index'][0])\n",
    "  return clstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_clusters(new_predict, dataframe):\n",
    "  \"\"\"\n",
    "  Pass in your new predictions and your dataframe \n",
    "  and it will renumber all of your clusters. \n",
    "  Be sure to use this before making visuals, if you don't\n",
    "  clusters that have been reassigned will just appear to be missing.\n",
    "  Edits your dataframe inplace. This is intentionally slower in \n",
    "  the hopes of not crashing my ram by iterating over each line one by one\n",
    "  \"\"\"\n",
    "  _clusters = [int(x) for x in np.unique(new_predict)]\n",
    "  _range = list(range(len(_clusters)))\n",
    "  to_rename = {}\n",
    "  for num in _range:\n",
    "    to_rename[_clusters[num]] = num\n",
    "  for i in range(len(dataframe)):\n",
    "    dataframe.at[i, 'cluster'] = to_rename[dataframe.at[i, 'cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_clst(df, dataframe, k, num_clust):\n",
    "  \"\"\"\n",
    "  Pass in your new predictions and your dataframe \n",
    "  and it will reassign the cluster you modeled to its new\n",
    "  cluster assignments. Edits dataframe inplace. \n",
    "  \"\"\"\n",
    "  for i in [x for x in df.index]:\n",
    "    cluster = df.at[i, 'cluster'] + k + 1 * num_clust\n",
    "    _id = df.at[i, 'id']\n",
    "    index = dataframe.loc[dataframe['id'] == _id].index[0]\n",
    "    dataframe.at[index, 'cluster'] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterizer(matrix, dataframe, calc_k):\n",
    "  \"\"\"\n",
    "  Pass in your feature array/matrix and your dataframe\n",
    "  If you would like to have k be calculated for you using the \n",
    "  elbow method with a maximum number of 5 clusters, do so by\n",
    "  setting calc_k = True, otherwise, k will be 3 \n",
    "  \"\"\"\n",
    "  global k \n",
    "  if calc_k == True:\n",
    "    k = plt_elbow(matrix, dataframe, False)\n",
    "  else: \n",
    "    k = 3 \n",
    "  if type(matrix) != np.ndarray:\n",
    "    matrix = matrix.toarray()\n",
    "  shape = matrix.shape[0]\n",
    "  kmeans = faiss.Kmeans(d = matrix.shape[1], k = k, nredo = 250,\n",
    "                        update_index = True, seed = 42, \n",
    "                        max_points_per_centroid = math.ceil(shape/k),\n",
    "                        min_points_per_centroid = math.floor(shape/k), \n",
    "                        niter = 20)\n",
    "  kmeans.train(matrix)\n",
    "  predict = kmeans.index.search(matrix, 1)[1]\n",
    "  dataframe['cluster'] = predict\n",
    "  return kmeans, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slice(df, cluster):\n",
    "  '''\n",
    "  Pass in your dataframe and return just the values predicted to be in the\n",
    "  largest cluster. \n",
    "  '''\n",
    "  return df.loc[df['cluster'] == cluster].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def showconfusionmatrix(y_t, y_hat_t, title):\n",
    "#     \"\"\"\n",
    "#     Plots confusion matrix for provided y_train, y_hat_train \n",
    "#     OR y_test, y_hat_test \n",
    "#     \"\"\"\n",
    "#     fig, ax = plot_confusion_matrix(confusion_matrix(y_t, y_hat_t))\n",
    "#     ax.set_title(f'{title} Data')\n",
    "#     ax.set_xticks([0, 1])\n",
    "#     ax.set_xticklabels(['True', 'False'])\n",
    "#     ax.set_yticks([0, 1])\n",
    "#     ax.set_yticklabels(['True', 'False'])\n",
    "#     ax.set_ylabel('Actual Data')\n",
    "#     ax.set_xlabel('Predicted Data')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printreports(y_test, y_hat_test, y_train, y_hat_train):\n",
    "    \"\"\"\n",
    "    Provided all input & predicted y values, prints classification \n",
    "    report for training and testing data right next to each other\"\"\"\n",
    "    print('                                                      ')\n",
    "    print('               Testing Report:')\n",
    "    report1 = classification_report(y_test, y_hat_test)\n",
    "    print(report1)\n",
    "    print('_____________________________________________________')\n",
    "    print('               Training Report:')\n",
    "    report2 = classification_report(y_train, y_hat_train)\n",
    "    print(report2)\n",
    "    print('_____________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a pipeline\n",
    "def pipeline(name_of_pipeline, classifier, X_train, y_train, X_test, y_test):\n",
    "    '''Creates and displays the pipeline classifiers along with the report of metrics'''\n",
    "    name_of_pipeline = Pipeline([('classifier', classifier)])\n",
    "    name_of_pipeline.fit(X_train, y_train)\n",
    "    y_pred_test = name_of_pipeline.predict(X_test)\n",
    "    y_pred_train = name_of_pipeline.predict(X_train)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    print(df)\n",
    "    print('\\n\\n')\n",
    "    print(name_of_pipeline.fit(X_train, y_train))\n",
    "    print('\\n\\n')\n",
    "    print('Training Accuracy: ', round(accuracy_score(y_train, y_pred_train),3))\n",
    "    print('Testing Accuracy: ', round(accuracy_score(y_test, y_pred_test),3))\n",
    "    print('\\n\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizing_confusionmatrix(name_of_pipeline, classifier, X_train, y_train, X_test, y_test):\n",
    "    '''Creates confusion matrices of the results from classifier'''\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
    "    name_of_pipeline = Pipeline([('classifier', classifier)])\n",
    "    name_of_pipeline.fit(X_train, y_train)\n",
    "    y_pred_test = name_of_pipeline.predict(X_test)\n",
    "    y_pred_train = name_of_pipeline.predict(X_train)\n",
    "    \n",
    "    #Plot Training Confusion Matrix\n",
    "    plot_confusion_matrix(classifier, X_train, y_train, ax=axes[0,0],\n",
    "                          display_labels=[\"0\",\"1\",\"2\",\"3\",\"4\"])\n",
    "    cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "    \n",
    "    #Plot Normalized Training Confusion Matrix\n",
    "    plot_confusion_matrix(classifier, X_train, y_train, ax=axes[1,0], \n",
    "                          display_labels=[\"0\",\"1\",\"2\",\"3\",\"4\"],\n",
    "                          normalize='true')\n",
    "    \n",
    "    #Plot Test Confusion Matrix\n",
    "    plot_confusion_matrix(classifier, X_test, y_test, ax=axes[0,1],\n",
    "                          display_labels=[\"0\",\"1\",\"2\",\"3\",\"4\"])\n",
    "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "    \n",
    "    #Plot Normalized Test Confusion Matrix\n",
    "    plot_confusion_matrix(classifier, X_test, y_test, ax=axes[1,1], \n",
    "                          display_labels=[\"0\",\"1\",\"2\",\"3\",\"4\"],\n",
    "                          normalize='true')\n",
    "    \n",
    "    axes[0,0].title.set_text(f'{classifier} Train')\n",
    "    axes[0,1].title.set_text(f'{classifier} Test')\n",
    "    axes[1,0].title.set_text(f'{classifier} Train')\n",
    "    axes[1,1].title.set_text(f'{classifier} Test')\n",
    "    \n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning | Model Setup\n",
    "\n",
    "First, we begin by loading our data and preprocessing. Since our data has already been cleaned, tokenized, and lemmatized, the main focus is placed on feature generation and dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data, low_memory = False because we have some mixed dtype columns, \n",
    "# they aren't one's we use anyways so not worried sbout it \n",
    "df = pd.read_csv(\"final_clean_6_word.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and preprocess our data, turning it into a matrix \n",
    "matrix, df = vect_and_stck(df, True, False)\n",
    "# taking an initial count to see how much we lose by the end \n",
    "start = [len(df), datetime.now()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Model | First Edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start our model & make our initial predictions \n",
    "first_means, predict = clusterizer(matrix, df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a number to help us determine if any of our clusters has too many points in it \n",
    "findimb = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_silhouette(df, matrix, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Model Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "num_clust = k\n",
    "cl = get_imb_clstr(df)\n",
    "# if our largest cluster is too large, we break it down into more clusters\n",
    "while len(get_slice(df, cl)) >= len(df)/num_clust-1 and num_clust <= 9:\n",
    "  # dropping outliers and shuffling our data  \n",
    "  df = chunked_samples(df, matrix, predict, True)\n",
    "  # processing our data without outliers \n",
    "  matrix, df = vect_and_stck(df, False, True)\n",
    "  # re-modeling our data with less outliers \n",
    "  bttr_means, predict = clusterizer(matrix, df, False)\n",
    "  # slicing our dataframe to get disproportionate cluster\n",
    "  cl = get_imb_clstr(df)\n",
    "  slice_matrix, slice_df = vect_and_stck(get_slice(df, cl), False, True)\n",
    "  # plugging our new k value into a new model optimized for our cluster \n",
    "  clstr_means, clstr_pred = clusterizer(slice_matrix, slice_df, True)\n",
    "  # updating our original datframe with our new clusters clusters\n",
    "  add_new_clst(slice_df, df, k, num_clust)\n",
    "  num_clust += k-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1).reset_index(drop=True)\n",
    "final_matrix, final_df = vect_and_stck(df, False, False)\n",
    "final_clusters = np.ravel(final_df['cluster'].to_numpy())\n",
    "reset_clusters(final_clusters, final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clusters = final_df['cluster'].to_numpy()\n",
    "# quick_silhouette(final_df, final_matrix, final_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = [len(final_df), datetime.now()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "gnb = make_pipeline(TfidfVectorizer(),  GaussianNB(var_smoothing=1e-20))\n",
    "bnb = make_pipeline(TfidfVectorizer(), BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [x for x in final_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(final_matrix, final_clusters, random_state=0)\n",
    "print(f' {round(end[0]/start[0], 4)}% Unclassifiable')\n",
    "time = end[1]-start[1]\n",
    "print(f' Model took {time.total_seconds()/60} min to complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline(gnb, GaussianNB(var_smoothing=1e-20), X_train, y_train, X_test, y_test)\n",
    "visualizing_confusionmatrix(gnb, GaussianNB(var_smoothing=1e-20), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(bnb, BernoulliNB(), X_train, y_train, X_test, y_test)\n",
    "visualizing_confusionmatrix(bnb, BernoulliNB(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnb = GaussianNB(var_smoothing=1e-20)\n",
    "\n",
    "# y_hat_test = gnb.fit(X_train, y_train).predict(X_test)\n",
    "# y_hat_train = gnb.fit(X_train, y_train)\n",
    "# showconfusionmatrix(y_test, y_hat_test, 'GNB')\n",
    "# printreports(y_test, y_hat_test, y_train, y_hat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bnb = BernoulliNB() \n",
    "\n",
    "# y_hat_test = bnb.fit(X_train, y_train).predict(X_test)\n",
    "# y_hat_train = bnb.fit(X_train, )\n",
    "# showconfusionmatrix(y_test, y_hat_test, 'BNB')\n",
    "# printreports(y_test, y_hat_test, y_train, y_hat_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
