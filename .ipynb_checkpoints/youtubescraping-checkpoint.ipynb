{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "novel-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afraid-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(vid_id, page):\n",
    "    \"\"\"Get response request for comments of a YouTube video from the Google API with given video ID and page ID\"\"\"\n",
    "    #setting up API auth\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    api_key = \"AIzaSyCMl5x85P9dgGgn4mADc-5RVLXdVh9Jczo\"\n",
    "    #I'm not sure what this is doing but it's in the documentation as what to do?\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey = api_key)\n",
    "    #setting up our request\n",
    "    req = youtube.commentThreads().list(\n",
    "        part=\"snippet,replies\",\n",
    "        videoId=vid_id,\n",
    "        maxResults= 100,\n",
    "        pageToken= page)\n",
    "    # getting response \n",
    "    response = req.execute()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "careful-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(num_comments, video_id):\n",
    "    \"\"\"Pass in number of comments requested (in hundreds) & the video_id (end of url excluding timestamp), \n",
    "    returns requested comments as pandas DataFrame\"\"\"\n",
    "    # figuring out how many iterations are needed from number of comments requested\n",
    "    iterations = int(num_comments/100)\n",
    "    # setting up our dictionary we will later convert to DataFrame\n",
    "    dict_ = {'text': [], 'vid_id':[], 'likes': [], 'date': [], 'channel_id': [], 'viewer_rating':[], 'mentions':[], 'comment_id':[]}\n",
    "    # later on we will check if the dataframe is still empty (like we are assigning it here) or if we need to add to a created one\n",
    "    df = ''\n",
    "    # when starting we do not have a next page, but we will re-write this as we iterate \n",
    "    nextpage = None\n",
    "    # start iterating\n",
    "    for i in range(iterations):\n",
    "        #wrap it in a try statment so that if we ask for too many comments and get an error it stops & still returns our df\n",
    "        try:\n",
    "            # get reponse for page \n",
    "            comments = get_response(video_id, nextpage)\n",
    "            count = 0 \n",
    "            # getting our coments and pages from our response \n",
    "            for item in comments.values():\n",
    "                count = count + 1 \n",
    "                if count == 3:\n",
    "                    nextpage = item\n",
    "                if count == 5:\n",
    "                    comments = item\n",
    "            for i in range(len(comments)):\n",
    "                # extracting data wanted from comments \n",
    "                dict_['text'].append(comments[i]['snippet']['topLevelComment']['snippet']['textOriginal'])\n",
    "                dict_['vid_id'].append(comments[i]['snippet']['videoId'])\n",
    "                dict_['likes'].append(comments[i]['snippet']['topLevelComment']['snippet']['likeCount'])\n",
    "                dict_['date'].append(comments[i]['snippet']['topLevelComment']['snippet']['publishedAt'])\n",
    "                dict_['channel_id'].append(comments[i]['snippet']['topLevelComment']['snippet']['authorChannelId']['value'])\n",
    "                dict_['viewer_rating'].append(comments[i]['snippet']['topLevelComment']['snippet']['viewerRating'])\n",
    "                dict_['comment_id'].append(comments[i]['id'])\n",
    "                dict_['mentions'].append('')\n",
    "                # getting the number of replies \n",
    "                try:\n",
    "                    replies = len(comments[i]['replies'])\n",
    "                except:\n",
    "                    replies = 0\n",
    "                # iterating over our replies and adding them to our dataframe \n",
    "                for j in range(replies):\n",
    "                    dict_['text'].append(comments[i]['replies']['comments'][j]['snippet']['textOriginal'])\n",
    "                    dict_['vid_id'].append(comments[i]['replies']['comments'][j]['snippet']['videoId'])\n",
    "                    dict_['likes'].append(comments[i]['replies']['comments'][j]['snippet']['likeCount'])\n",
    "                    dict_['date'].append(comments[i]['replies']['comments'][j]['snippet']['publishedAt'])\n",
    "                    dict_['channel_id'].append(comments[i]['replies']['comments'][j]['snippet']['authorChannelId']['value'])\n",
    "                    dict_['viewer_rating'].append(comments[i]['replies']['comments'][j]['snippet']['viewerRating'])              \n",
    "                    dict_['mentions'].append(comments[i]['replies']['comments'][j]['snippet']['parentId'])\n",
    "                    dict_['comment_id'].append(comments[i]['replies']['comments'][j]['id'])\n",
    "        except:\n",
    "            pass\n",
    "    # converting our dictionary into a DataFrame and returning it \n",
    "    df = pd.DataFrame.from_dict(dict_)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "spare-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of videos to get the comments from \n",
    "video_list = ['PQnvjGN91Mg']\n",
    "# starting a dataframe for us to add additional comments onto \n",
    "start_df = get_comments(1000, 'BI-old7YI4I')\n",
    "\n",
    "#iterating over our list of videos to get their comments and add them to our starting dataframe \n",
    "for item in video_list:\n",
    "    start_df = start_df.append(get_comments(1000, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "advanced-tobago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>vid_id</th>\n",
       "      <th>likes</th>\n",
       "      <th>date</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>viewer_rating</th>\n",
       "      <th>mentions</th>\n",
       "      <th>comment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You lost me at spinning up a planet. If we cou...</td>\n",
       "      <td>BI-old7YI4I</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-17T11:52:29Z</td>\n",
       "      <td>UCnPr4Ay0ICYWHMzrjbpxvvA</td>\n",
       "      <td>none</td>\n",
       "      <td></td>\n",
       "      <td>Ugwsvy3QkP99mBvF_r14AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh wait it got worse, moving a moon? LOL.</td>\n",
       "      <td>BI-old7YI4I</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-17T11:53:55Z</td>\n",
       "      <td>UCnPr4Ay0ICYWHMzrjbpxvvA</td>\n",
       "      <td>none</td>\n",
       "      <td>Ugwsvy3QkP99mBvF_r14AaABAg</td>\n",
       "      <td>Ugwsvy3QkP99mBvF_r14AaABAg.9Kzi3mj4FR09KziEHyLgwz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My parents are born in the 1940 and 1950s and ...</td>\n",
       "      <td>BI-old7YI4I</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-14T01:47:13Z</td>\n",
       "      <td>UCJ9rKpS9E4hw7mGdjK58klw</td>\n",
       "      <td>none</td>\n",
       "      <td></td>\n",
       "      <td>UgxxDjQoUZFQ-B-v2wV4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lol. In a hundred years there will be barely a...</td>\n",
       "      <td>BI-old7YI4I</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-12T19:23:13Z</td>\n",
       "      <td>UCGVSgYEs7NUvAfCFwH9zmqA</td>\n",
       "      <td>none</td>\n",
       "      <td></td>\n",
       "      <td>Ugyr-GSf2sxBltBkr3N4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I will send all Americans in to\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>BI-old7YI4I</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-11T20:20:52Z</td>\n",
       "      <td>UCMTVFn7L_FDOnpu3AnwYbRQ</td>\n",
       "      <td>none</td>\n",
       "      <td></td>\n",
       "      <td>UgyDuOHwHf1FJaEuUjV4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>I like your Content,but should not first fix o...</td>\n",
       "      <td>PQnvjGN91Mg</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-23T16:12:41Z</td>\n",
       "      <td>UC9ijt06rZWaNHq-u3k7A-aA</td>\n",
       "      <td>none</td>\n",
       "      <td></td>\n",
       "      <td>UgwDIDJtdFziG78ELip4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>No. If people like you had your way, Scott nev...</td>\n",
       "      <td>PQnvjGN91Mg</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-24T00:36:05Z</td>\n",
       "      <td>UC7TdXaCru7Doy3rSwafySWQ</td>\n",
       "      <td>none</td>\n",
       "      <td>UgwDIDJtdFziG78ELip4AaABAg</td>\n",
       "      <td>UgwDIDJtdFziG78ELip4AaABAg.8_JWzodrOPz8_KQat3bm6h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>what about the slylandro</td>\n",
       "      <td>PQnvjGN91Mg</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-23T16:12:09Z</td>\n",
       "      <td>UCUfI9cyI1XOrSFySIcFLiwA</td>\n",
       "      <td>none</td>\n",
       "      <td></td>\n",
       "      <td>Ugw6f-j5v7mSLzM-9FV4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>Any chance of an outward bound: Asteroid Belt ...</td>\n",
       "      <td>PQnvjGN91Mg</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-11-23T16:11:09Z</td>\n",
       "      <td>UCwi6ZTWYkyI2lNV7JvAL5qg</td>\n",
       "      <td>none</td>\n",
       "      <td></td>\n",
       "      <td>UgwRFiRzFtXIvdeTAA54AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>I give thanks to this new episode of outward b...</td>\n",
       "      <td>PQnvjGN91Mg</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-23T16:05:03Z</td>\n",
       "      <td>UCSCNw6u-Cgipwkn4NdcMq5g</td>\n",
       "      <td>none</td>\n",
       "      <td></td>\n",
       "      <td>UgxQgVkRBeKiq5thGF94AaABAg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2260 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text       vid_id  likes  \\\n",
       "0     You lost me at spinning up a planet. If we cou...  BI-old7YI4I      0   \n",
       "1             Oh wait it got worse, moving a moon? LOL.  BI-old7YI4I      0   \n",
       "2     My parents are born in the 1940 and 1950s and ...  BI-old7YI4I      0   \n",
       "3     Lol. In a hundred years there will be barely a...  BI-old7YI4I      0   \n",
       "4     I will send all Americans in to\\n\\n\\n\\n\\n\\n\\n\\...  BI-old7YI4I      0   \n",
       "...                                                 ...          ...    ...   \n",
       "1145  I like your Content,but should not first fix o...  PQnvjGN91Mg      0   \n",
       "1146  No. If people like you had your way, Scott nev...  PQnvjGN91Mg      1   \n",
       "1147                           what about the slylandro  PQnvjGN91Mg      0   \n",
       "1148  Any chance of an outward bound: Asteroid Belt ...  PQnvjGN91Mg      3   \n",
       "1149  I give thanks to this new episode of outward b...  PQnvjGN91Mg      0   \n",
       "\n",
       "                      date                channel_id viewer_rating  \\\n",
       "0     2021-03-17T11:52:29Z  UCnPr4Ay0ICYWHMzrjbpxvvA          none   \n",
       "1     2021-03-17T11:53:55Z  UCnPr4Ay0ICYWHMzrjbpxvvA          none   \n",
       "2     2021-03-14T01:47:13Z  UCJ9rKpS9E4hw7mGdjK58klw          none   \n",
       "3     2021-03-12T19:23:13Z  UCGVSgYEs7NUvAfCFwH9zmqA          none   \n",
       "4     2021-03-11T20:20:52Z  UCMTVFn7L_FDOnpu3AnwYbRQ          none   \n",
       "...                    ...                       ...           ...   \n",
       "1145  2017-11-23T16:12:41Z  UC9ijt06rZWaNHq-u3k7A-aA          none   \n",
       "1146  2017-11-24T00:36:05Z  UC7TdXaCru7Doy3rSwafySWQ          none   \n",
       "1147  2017-11-23T16:12:09Z  UCUfI9cyI1XOrSFySIcFLiwA          none   \n",
       "1148  2017-11-23T16:11:09Z  UCwi6ZTWYkyI2lNV7JvAL5qg          none   \n",
       "1149  2017-11-23T16:05:03Z  UCSCNw6u-Cgipwkn4NdcMq5g          none   \n",
       "\n",
       "                        mentions  \\\n",
       "0                                  \n",
       "1     Ugwsvy3QkP99mBvF_r14AaABAg   \n",
       "2                                  \n",
       "3                                  \n",
       "4                                  \n",
       "...                          ...   \n",
       "1145                               \n",
       "1146  UgwDIDJtdFziG78ELip4AaABAg   \n",
       "1147                               \n",
       "1148                               \n",
       "1149                               \n",
       "\n",
       "                                             comment_id  \n",
       "0                            Ugwsvy3QkP99mBvF_r14AaABAg  \n",
       "1     Ugwsvy3QkP99mBvF_r14AaABAg.9Kzi3mj4FR09KziEHyLgwz  \n",
       "2                            UgxxDjQoUZFQ-B-v2wV4AaABAg  \n",
       "3                            Ugyr-GSf2sxBltBkr3N4AaABAg  \n",
       "4                            UgyDuOHwHf1FJaEuUjV4AaABAg  \n",
       "...                                                 ...  \n",
       "1145                         UgwDIDJtdFziG78ELip4AaABAg  \n",
       "1146  UgwDIDJtdFziG78ELip4AaABAg.8_JWzodrOPz8_KQat3bm6h  \n",
       "1147                         Ugw6f-j5v7mSLzM-9FV4AaABAg  \n",
       "1148                         UgwRFiRzFtXIvdeTAA54AaABAg  \n",
       "1149                         UgxQgVkRBeKiq5thGF94AaABAg  \n",
       "\n",
       "[2260 rows x 8 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting rid of duplicate entries (if any)\n",
    "start_df = start_df.drop_duplicates(subset='comment_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
