{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "close-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "from googleapiclient.errors import HttpError\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "collect-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = googleapiclient.discovery.build(\n",
    "    \"youtube\", \"v3\",\n",
    "    developerKey = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entitled-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_from_video(video_id: str, max_comments = 0) -> None:\n",
    "    \"\"\"\n",
    "    Takes the video's ID, the string at the end of the url, after \"v=\",\n",
    "    and compiles the comments, appending them to a global dataframe df.\n",
    "\n",
    "    Optionally accepts a limit to comments gathered, default value of\n",
    "    zero returns all comments. If you specify a max, you may still get\n",
    "    more than specified due to how the API responds.\n",
    "\n",
    "    Comments are returned with the most recently interacted\n",
    "    with first, such as a brand new comment, or an old comment with a new\n",
    "    reply added. \n",
    "    \"\"\"\n",
    "    # Define a function to reset dict_ with, to facilitate appending the dataframe\n",
    "    def reset_dict():\n",
    "        return {\n",
    "            'video_id':[], 'text': [], 'likes': [],\n",
    "            'date': [], 'channel_id': [], 'viewer_rating':[],\n",
    "            'mentions':[], 'comment_id':[]\n",
    "            }\n",
    "    # Initialize the dict_ and access the global df reference\n",
    "    dict_ = reset_dict()\n",
    "    global df\n",
    "\n",
    "    # Adds relevent data from the comment thread's top comment to the dict_\n",
    "    def read_top_level_comment(comment):\n",
    "        dict_['video_id'].append(comment['snippet']['videoId'])\n",
    "        dict_['text'].append(comment['snippet']['topLevelComment']['snippet']['textOriginal'])\n",
    "        dict_['likes'].append(comment['snippet']['topLevelComment']['snippet']['likeCount'])\n",
    "        dict_['date'].append(comment['snippet']['topLevelComment']['snippet']['publishedAt'])\n",
    "        dict_['channel_id'].append(comment['snippet']['topLevelComment']['snippet']['authorChannelId']['value'])\n",
    "        dict_['viewer_rating'].append(comment['snippet']['topLevelComment']['snippet']['viewerRating'])\n",
    "        dict_['mentions'].append('')\n",
    "        dict_['comment_id'].append(comment['id'])\n",
    "\n",
    "    # Adds the relevent data to a reply comment to the dict_\n",
    "    # Is a seperate function, as the data is located in slightly different locations\n",
    "    def read_reply(reply):\n",
    "        # Reply comments do not carry their own videoId reference, so you have to copy\n",
    "        # the previous to get the videoId from the top level comment\n",
    "        dict_['video_id'].append(dict_['video_id'][-1])\n",
    "        dict_['text'].append(reply['snippet']['textOriginal'])\n",
    "        dict_['likes'].append(reply['snippet']['likeCount'])\n",
    "        dict_['date'].append(reply['snippet']['publishedAt'])\n",
    "        dict_['channel_id'].append(reply['snippet']['authorChannelId']['value'])\n",
    "        dict_['viewer_rating'].append(reply['snippet']['viewerRating'])\n",
    "        dict_['mentions'].append(reply['snippet']['parentId'])\n",
    "        dict_['comment_id'].append(reply['id'])\n",
    "\n",
    "    # Determine if a max number of comments is called for and create initial request\n",
    "    max_comments = max_comments if max_comments > 0 else float('inf')\n",
    "    request = youtube.commentThreads().list(\n",
    "        part = \"snippet,replies\",\n",
    "        videoId = video_id,\n",
    "        maxResults = min(100, max_comments))\n",
    "    \n",
    "    # Loop persists until all comments gathered, max is exceeded, or comments cannot be accessed\n",
    "    while request is not None and max_comments > 0:\n",
    "        # Try most recent request, ending function if an error occurs\n",
    "        try:\n",
    "            response = request.execute()\n",
    "        except HttpError as err:\n",
    "            print(err)\n",
    "            return\n",
    "\n",
    "        # After positive response, loop through the list of top level comments\n",
    "        for comment_thread in response['items']:\n",
    "            read_top_level_comment(comment_thread)\n",
    "            # If there are no replies to the top level comment, skip to next iteration\n",
    "            if comment_thread['snippet']['totalReplyCount'] < 1:\n",
    "                continue\n",
    "            # If there are up to five replies, they are all included in \n",
    "            # the original response and can be read directly before moving on\n",
    "            if comment_thread['snippet']['totalReplyCount'] <= 5:\n",
    "                for reply in comment_thread['replies']['comments']:\n",
    "                    read_reply(reply)\n",
    "                continue\n",
    "            # Finally, if there are more than five replies, a new request must\n",
    "            # be made to retrieve them all\n",
    "            reply_request = youtube.comments().list(\n",
    "                part = \"snippet\",\n",
    "                parentId = comment_thread['id'],\n",
    "                maxResults = 100)\n",
    "            # A new loop is necessary to retrieve replies, in case they exceed 100\n",
    "            while reply_request is not None:\n",
    "                # Try most recent reply request, breaking the loop if there is an\n",
    "                # HttpError. It is likely that this loop will be what causes you to\n",
    "                # exceed your daily limit, so we will just move on from any bad response\n",
    "                try:\n",
    "                    reply_response = reply_request.execute()\n",
    "                except HttpError as err:\n",
    "                    break\n",
    "                for reply in reply_response['items']:\n",
    "                    read_reply(reply)\n",
    "                reply_request = youtube.comments().list_next(reply_request, reply_response)\n",
    "\n",
    "        # Decrements max if necessary and create request for next page if possible\n",
    "        max_comments -= len(dict_['text'])\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "\n",
    "        # Append the dataframe with the comments gathered in this loop and reset the dict for the next loop\n",
    "        df = df.append(pd.DataFrame.from_dict(dict_)).reset_index(drop=True)\n",
    "        dict_ = reset_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tight-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list containing all of the videos we've scraped so that we don't waste any time re-scraping them \n",
    "video_list =   ['JWeR_F4uyE0', 'VIMV6E8OxG8', 'THqtAQOicQI', '6VBCxWcAPXw', 'PQnvjGN91Mg',\n",
    "                    'kmFOBoy2MZ8', '76sJ7C0QEJs', 'C30gxc6TWuY', 'W9olSzNOh8s', \n",
    "                    't_n0yhhuJBs', 'NtQkz0aRDe8', '-9lBVznUuHk', 'X8bBP_cLrl0', 'b3D7QlMVa5s', \n",
    "                    'wYDJ0vxg1lU', 'NtQkz0aRDe8', '-dL28N5yPmQ', 'R_LqgcndmAo', 'eH-xm9G9QBk', 'kS0Jg6hlUSs', \n",
    "                    'eXRdZ_qnZTA', '2emC9xPKh_Q', 'h8T9mVkGh3s', 'gWKyTYEFVGY', '-YebEDmbG_M', 'llh8rfwWqqY', \n",
    "                    '60V0_-AHfyM', 'w6J7FteaW2Q', 'H0CBLw0xOlo', 'QKq4sLERZ4M', '9T6WqdHq7JY', 'kS1J-ZSaecw', \n",
    "                    'w_4D6xKqH9w', 'D1KPZOK-iHg', 'ji5i4gXBcSk', 'LdIW_bOaspg', 'AKLnXeFDQ1A', 'TOivsknjD0k', \n",
    "                    'njESY1JxNcM', 'P5BNNA97LEc', 'VOD_uugAlJw', 'xg_jyUDsLpU', 'N_SjGaiuGoU', '1xWbCcaJnIQ', \n",
    "                    'db4cEuLpPsQ', 'aTci511TD4A', 'ego91VOyObw', '88QDCJkNLlE', 'LOkqR4CK7Qc', 'hgE-v1OEJFM', \n",
    "                    'hzp7vqgprCc', 'uFhsagtKtwM', 'QKq4sLERZ4M', 'JzeYsRt7axc', 'nhimQHsTo0s', '8ydvxFu6bJ8', \n",
    "                    '9ot3bCkhjTM', 'mKAIL8DDemg', 'kPd56OY2ED8', 'FTcXKFZcToM', '-R2x02n-o64', 'vS7aidy2bwk', \n",
    "                    'iB0ilH7yrfU', 'XVqPwcnRGBU', 'OyrFddzsymQ', '0kZ-EcGt39s', '0pGzSKohRJo', 'e7o4ct0Z8tI',\n",
    "                    'VYD0DleJn7U', 'OYAgcS31-p0', 'Zo62S0ulqhA', '5OLtteIwwNs', 'ZL4yYHdDSWs', 'UNEFDynNw-Q',\n",
    "                    't59Ge4O70iM', 'AhF44UT2AIk', 'biSWmzIg-2k', 'N-1gzo3Pyvo', '0kZ-EcGt39s', 'OyF1ByhjSv0', \n",
    "                    'g_ROkapCj14', 'Bsgrbd_Yv4Y', 'fXcmzmWXZmw', 'fzyYAVz3IGg', 'ep3GlvxUUew', '2QI7z46LWLY', \n",
    "                    '8Oh5ARY_MCM', 'fniq8Wuw60A', 'H0CBLw0xOlo', '5nE3UO1kqv0', '-nwbLls-PCs', 'vTNP01Sg-Ss', \n",
    "                    'PHY_vAKLzzo', 'BpPmP8DUh4s', 'F-c5iAyfgAU', 'lQS8cnsZuGU', 'LmfaVwAXZy4', 'wtlUnI1fe8Y', \n",
    "                    '35b2tAMxQXg', 'hTbQF6UBe5Q', 'IW9A-uWM0JU', '85vvVZ4jSZM', 'xudplVZgGV0', '76sJ7C0QEJs', \n",
    "                    '8Uvgh4gYzlw', 'ySKIm7k1-18', 'oAqhNmLmY7g', 'C30gxc6TWuY', '1iGriklFHHQ', 'GQ7v2dI2RF4', \n",
    "                    'lCCKdcL_h3Y', 'oBXmUP3Jq8A', 'SM1vXb6J7gE', 'dTEIL19FLYI', 'VZpN7hd1ybI', 'C9GiZDoZvxE',\n",
    "                    '-qov7HlrvbM', 'KZXjFdrct-w', 'NyLPPXaGl5A', 'C2jh7dCwGRs', 'XL1ehbG9EL8', '42Je9Xczu0o', \n",
    "                    'D-J9maAnhwg', '_Ihdb8-h5Ek', '4cv3SjVK-n0', 'hYyg8JC-6ew', 'RcXBuYwm3xk', '-YebEDmbG_M', \n",
    "                    'TNRQFKVV68I', 'pxa0IrZCNzg', 'vFdx1Hs71iA', 'fM-JHvg-ZCM', 'aCCR5qBsD0c', 'cb6sdimG8GE', \n",
    "                    '0ENabNTQwNg', 'LqoYtBZAKO0', 'H2f0Wd3zNj0', 'JkeLIAd2Nd0', 'TmLWxptFFYc', 'S0dqd72ALkQ', \n",
    "                    '0Ap4JhPoPQY', 'P4aXmnQzJ0o', 'PQnvjGN91Mg', 'HdpRxGjtCo0', 'BI-old7YI4I', 'kmFOBoy2MZ8', \n",
    "                    'bGcvv3683Os', 'JgxkilF5XUM', 's6BQSgidbmc', '6VBCxWcAPXw', '2zaIy1TARPE', '3y3MmmfZmP8', \n",
    "                    'xe4Kkbq4An8', 'X4C5fbcYSNg', 'U09K0bQT5PE', 'X8bBP_cLrl0', 'oyKnBTIoC5E', 'EVicgFd25D4',\n",
    "                    'Ox6pqjQiuJ0', 'fwCl9Ce7MDM', 'aPuDNDZZ6-U', '_9MKYKR8lFA', 'vOpH3xnzFJE', 'bq220dgUb0I', \n",
    "                    'lLTdBJsU8N8', 'qXZdRDoGSHo', 'I7yCAmLEDdo', 'Gogn3p8aDEs', 'TYB8dvCNCQc', 'g_m5VRiKy_E', \n",
    "                    'Gcnf5BdLXxw', '1bJKAu11Ni4', 'OYAgcS31-p0', 'PPqI-Sk7vsw', 'YWKWkuJwHj4', 'JVhJcXBTl3Y', \n",
    "                    'wfAoq89LNRQ', 'ZSNxaWkuoRo', '4cvZ9NWgsws', '-n9uz_cOjT8', '17i2kyEgjWE', '5nE3UO1kqv0',\n",
    "                    'JmF-OOuOxKg', 'WREUb8T4r8o', 'Li7_yFiNaIA', 'FxrAe5N1xu0', 'CIf6VJH4dZk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "informational-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of videos to get the comments from \n",
    "completed_videos = []\n",
    "# starting a dataframe for us to add additional comments onto \n",
    "df = pd.DataFrame()\n",
    "\n",
    "#iterating over our list of videos to get their comments and add them to our starting dataframe \n",
    "for item in video_list:\n",
    "    # checking if video is in our completed list, easier than remembering or checking visually\n",
    "    if item not in completed_videos:\n",
    "        get_comments_from_video(video_id=item)\n",
    "        completed_videos.append(item)\n",
    "        print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "selected-cooking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206026"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hollywood-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('doublechecking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-commissioner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
