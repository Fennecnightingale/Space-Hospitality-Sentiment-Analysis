{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import twython\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "specified-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "associate-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "from contextlib import suppress\n",
    "from requests_oauthlib import OAuth1Session\n",
    "from apscheduler.schedulers.background import BackgroundScheduler as Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chronic-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your keys/secrets as strings in the following fields\n",
    "credentials = {}\n",
    "credentials['CONSUMER_KEY'] = \n",
    "credentials['CONSUMER_SECRET'] = \n",
    "credentials['ACCESS_KEY'] = \n",
    "credentials['ACCESS_SECRET'] = \n",
    "\n",
    "# Save the credentials object to file\n",
    "with open(\"twitter_credentials.json\", \"w\") as file:\n",
    "    json.dump(credentials, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "contrary-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials from json file\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)\n",
    "\n",
    "# Instantiate an object\n",
    "python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "verbal-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the header for our dataframe so we have something to append to \n",
    "startdf = pd.read_csv('df')\n",
    "# when our dataframe is created the index is unnamed, when you export it is renamed so we need to fix it so our df realizes it's the same \n",
    "startdf = startdf.head(0).rename(columns={'Unnamed: 0':''})\n",
    "#assign our id as the most recent tweet id to start so that we get everything after it \n",
    "id_ = python_tweets.search(**{'q': 'RT', 'result_type': 'recent', 'count': 2})['statuses'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adaptive-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfn(q, name):\n",
    "    # import global variables we will be using this function to alter\n",
    "    global id_\n",
    "    global startdf \n",
    "    # Create our query\n",
    "    query = {'q': q,\n",
    "            'result_type': 'recent',\n",
    "            'count': 100,\n",
    "            'lang': 'en',\n",
    "            'max_id': id_,\n",
    "            'tweet_mode' : 'extend',\n",
    "            'entities': {\n",
    "                \"hashtags\": [],\n",
    "                \"symbols\": [],\n",
    "                \"user_mentions\": []\n",
    "            }\n",
    "            }\n",
    "    #build our query into a dictionary to easily turn into dataframe \n",
    "    dict_ = {'user': [], 'user_id':[], 'post_id': [], 'text': [], 'favorite_count': [], 'hashtags':[], 'symbols':[], 'user_mentions':[], \n",
    "             'retweet_count':[]}\n",
    "    for status in python_tweets.search(**query)['statuses']:\n",
    "        dict_['user'].append(status['user']['screen_name'])\n",
    "        dict_['user_id'].append(status['user']['id'])\n",
    "        dict_['text'].append(status['text'])\n",
    "        dict_['hashtags'].append(status['entities']['hashtags'])\n",
    "        dict_['symbols'].append(status['entities']['symbols'])\n",
    "        dict_['user_mentions'].append(status['entities']['user_mentions'])\n",
    "        dict_['favorite_count'].append(status['favorite_count'])\n",
    "        dict_['retweet_count'].append(status['retweet_count'])\n",
    "        dict_['post_id'].append(status['id'])\n",
    "    # put data in a DataFrame to work with it easier\n",
    "    df = pd.DataFrame(dict_)\n",
    "    #removing our vairables stuck in dictionaries within our dataframe and give them their own columns\n",
    "    df['mentions'] = ' '\n",
    "    df['hashtag'] = ' '\n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(df.at[i, 'user_mentions'])):\n",
    "            try:\n",
    "                df.at[i, 'mentions'] = str(df.at[i, 'mentions']) + ' ' + str(df.at[i, 'user_mentions'][j]['id'])\n",
    "            except:\n",
    "                df.at[i, 'mentions'] = str(df.at[i, 'mentions'])\n",
    "        for k in range(len(df.at[i, 'hashtags'])):\n",
    "            try:\n",
    "                df.at[i, 'hashtag'] = str(df.at[i, 'hashtag']) + ' ' + str(df.at[i, 'hashtags'][k]['text'])\n",
    "            except:\n",
    "                df.at[i, 'hashtag'] = str(df.at[i, 'hashtag'])\n",
    "    # get rid of columns that we took everything we need from \n",
    "    df = df.drop(['user_mentions', 'hashtags'], 1)\n",
    "    startdf = startdf.append(df)\n",
    "    # not sure why id_ doesn't want to change unless reset but do it because it works and can't hurt\n",
    "    id_ = None\n",
    "    # get oldest id_ (lowest number) and subtract one so that that you don't reinclude it in the next search\n",
    "    # reset id_ to be oldest\n",
    "    try: \n",
    "        id_ = int(df['post_id'].sort_values(ascending=True).to_frame().reset_index()['post_id'][0]) - 1 \n",
    "    except:\n",
    "        id_ = python_tweets.search(**{'q': 'RT', 'result_type': 'recent', 'count': 2})['statuses'][0]['id']\n",
    "    return startdf, id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "opposed-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'astronomy', 'Starship', 'mars', 'curiosityrover', 'oppertunityrover', 'starlink', 'falconheavy', 'sls','ESA', 'NASA', 'spacex', 'virgingalactic', 'virginorbit', 'JAXA', 'Roscosmos', 'artemis', \n",
    "# 'starliner', 'blueorigin', 'spacetravel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "enabling-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = ['marswebcam', 'falcon9', 'nasa_app', 'universe', 'cosmos', 'iss', 'climate', 'internationalspacestation', 'futurism', 'starliner', 'blueorigin', 'spacetravel', 'astronomy', 'Starship', 'mars', 'curiosityrover', 'oppertunityrover', 'starlink', 'falconheavy', 'sls','ESA', 'NASA', 'spacex', 'virgingalactic', 'virginorbit', 'JAXA', 'Roscosmos', 'artemis', 'areospace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "immediate-service",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting marswebcam time : 15:18:58, ending time: 15:35:37\n",
      "starting falcon9 time : 15:35:38, ending time: 15:52:17\n",
      "starting nasa_app time : 15:52:18, ending time: 16:08:57\n",
      "starting universe time : 16:08:58, ending time: 16:25:37\n",
      "starting cosmos time : 16:25:38, ending time: 16:42:17\n",
      "starting iss time : 16:42:18, ending time: 16:58:57\n",
      "starting climate time : 16:58:58, ending time: 17:15:37\n",
      "starting internationalspacestation time : 17:15:38, ending time: 17:32:17\n",
      "starting futurism time : 17:32:18, ending time: 17:48:57\n",
      "starting starliner time : 17:48:58, ending time: 18:05:37\n",
      "starting blueorigin time : 18:05:38, ending time: 18:22:17\n",
      "starting spacetravel time : 18:22:18, ending time: 18:38:57\n",
      "starting astronomy time : 18:38:58, ending time: 18:55:37\n",
      "starting Starship time : 18:55:38, ending time: 19:12:17\n",
      "starting mars time : 19:12:18, ending time: 19:28:57\n",
      "starting curiosityrover time : 19:28:58, ending time: 19:45:37\n",
      "starting oppertunityrover time : 19:45:38, ending time: 20:02:17\n",
      "starting starlink time : 20:02:18, ending time: 20:18:57\n",
      "starting falconheavy time : 20:18:58, ending time: 20:35:37\n",
      "starting sls time : 20:35:38, ending time: 20:52:17\n",
      "starting ESA time : 20:52:18, ending time: 21:08:57\n",
      "starting NASA time : 21:08:58, ending time: 21:25:37\n",
      "starting spacex time : 21:25:38, ending time: 21:42:17\n",
      "starting virgingalactic time : 21:42:18, ending time: 21:58:57\n",
      "starting virginorbit time : 21:58:58, ending time: 22:15:37\n",
      "starting JAXA time : 22:15:38, ending time: 22:32:17\n",
      "starting Roscosmos time : 22:32:18, ending time: 22:48:57\n",
      "starting artemis time : 22:48:58, ending time: 23:05:37\n",
      "starting areospace time : 23:05:38, ending time: 23:22:17\n"
     ]
    }
   ],
   "source": [
    "# eventual goal is to use those loop to do all of our searches\n",
    "end_item = datetime.datetime.now()\n",
    "id_ = python_tweets.search(**{'q': 'RT', 'result_type': 'recent', 'count': 2})['statuses'][0]['id']\n",
    "\n",
    "for item in query_list:\n",
    "    #reset id_ to be most recent tweet for each item\n",
    "    id_ = python_tweets.search(**{'q': 'RT', 'result_type': 'recent', 'count': 2})['statuses'][0]['id']\n",
    "    # for each item, reset the start time to the end of the last item \n",
    "    start_time = end_item + datetime.timedelta(seconds=1)\n",
    "    # set the end time to be 90 min after you start, gathering 50 tweets every 100 seconds for 2,700 tweets from each item in list\n",
    "    end_item = end_item + datetime.timedelta(seconds=1000)\n",
    "    #print start and end times so I know when the program will be finished and where it should be at \n",
    "    print(f'starting {item} time : {start_time.strftime(\"%H:%M:%S\")}, ending time: {end_item.strftime(\"%H:%M:%S\")}')\n",
    "    #occassionally getting key/value errors of 0 that I cannot find the cause of, but do not affect anything in how our dataframe\n",
    "    #is made, I just don't want them printing \n",
    "    with suppress(KeyError, ValueError):\n",
    "        #initiate scheduler\n",
    "        sch = Scheduler()\n",
    "        #add our function and items, we're doing 50/10 seconds \n",
    "        sch.add_job(myfn, 'interval', (item, startdf), seconds= 8, start_date=start_time, end_date=end_item)\n",
    "        #start scheduler\n",
    "        sch.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bearing-insured",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(startdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('alltweets39')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newstartdf = startdf[['user', 'user_id', 'text', 'favorite_count', 'retweet_count',\n",
    "#                    'mentions', 'hashtag', 'post_id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.append(newstartdf)\n",
    "# df = df.drop_duplicates(subset='post_id')\n",
    "# df = df.sort_values(by='favorite_count', ascending=False).drop_duplicates(subset='text', keep='first')\n",
    "# df = df.reset_index()\n",
    "# print(len(df))\n",
    "# #171476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['hashtag'].value_counts().to_frame().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "polyphonic-accommodation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run time of job \"myfn (trigger: interval[0:00:08], next run at: 2021-03-20 22:48:58 PDT)\" was missed by 11:03:13.538887\n",
      "Run time of job \"myfn (trigger: interval[0:00:08], next run at: 2021-03-20 23:05:38 PDT)\" was missed by 11:03:13.541578\n"
     ]
    }
   ],
   "source": [
    "# startdf.to_csv('beeep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since every time we loop through it starts a new index, there are now multiple entries for each index, we need to fix this\n",
    "# and the easiest way is to just reset the index once it finishes running \n",
    "startdf = startdf.drop_duplicates(subset='post_id')\n",
    "startdf = startdf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need these columns anymore\n",
    "startdf = startdf.drop(['index', '', 'symbols'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to add our strings for our hashtags and mentions together we had to make the entire column a string, sinlucding empty cells\n",
    "# here we go through and replace empty cells with nan values so pandas will read them as being empty instead of a string\n",
    "for i in range(len(startdf)):\n",
    "    if startdf.at[i, 'mentions'] == ' ':\n",
    "        startdf.at[i, 'mentions'] = np.nan\n",
    "    else: \n",
    "        pass \n",
    "    if startdf.at[i, 'hashtag'] == ' ':\n",
    "        startdf.at[i, 'hashtag'] = np.nan\n",
    "    else: \n",
    "        pass "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
