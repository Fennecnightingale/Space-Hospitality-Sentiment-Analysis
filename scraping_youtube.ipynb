{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twython\n",
      "  Downloading twython-3.8.2-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: requests>=2.1.0 in /Users/tlipman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from twython) (2.24.0)\n",
      "Collecting requests-oauthlib>=0.4.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/tlipman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from requests>=2.1.0->twython) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/tlipman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from requests>=2.1.0->twython) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/tlipman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from requests>=2.1.0->twython) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tlipman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from requests>=2.1.0->twython) (2020.12.5)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 18.3 MB/s eta 0:00:01     |█████████████████████████████   | 133 kB 18.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: oauthlib, requests-oauthlib, twython\n",
      "Successfully installed oauthlib-3.1.0 requests-oauthlib-1.3.0 twython-3.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install twython\n",
    "!pip install apscheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import twython\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "from contextlib import suppress\n",
    "from requests_oauthlib import OAuth1Session\n",
    "from apscheduler.schedulers.background import BackgroundScheduler as Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS LINE OF CODE IS WHAT WE USED TO AUTHENTICATE AND MAKE REQUESTS\n",
    "\n",
    "# Enter your keys/secrets as strings in the following fields\n",
    "# credentials = {}\n",
    "# credentials['CONSUMER_KEY'] = \"consumer key\"\n",
    "# credentials['CONSUMER_SECRET'] = \"consumer token\"\n",
    "# credentials['ACCESS_KEY'] = \"access key\"\n",
    "# credentials['ACCESS_SECRET'] = \"access token\"\n",
    "\n",
    "# Save the credentials object to file\n",
    "# with open(\"twitter_credentials.json\", \"w\") as file:\n",
    "#    json.dump(credentials, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials from json file\n",
    "with open(\"/Users/tlipman/.secret/twitter_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)\n",
    "\n",
    "# Instantiate an object\n",
    "python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the header for our dataframe so we have something to append to \n",
    "startdf = pd.read_csv('df')\n",
    "# when our dataframe is created the index is unnamed, when you export it is renamed so we need to fix it so our df realizes it's the same \n",
    "startdf = startdf.head(0).rename(columns={'Unnamed: 0':''})\n",
    "#assign our id as the most recent tweet id to start so that we get everything after it \n",
    "id_ = python_tweets.search(**{'q': 'RT', 'result_type': 'recent', 'count': 2})['statuses'][0]['id']\n",
    "\n",
    "def myfn(q, name):\n",
    "    # import global variables we will be using this function to alter\n",
    "    global id_\n",
    "    global startdf \n",
    "    # Create our query\n",
    "    query = {'q': q,\n",
    "            'result_type': 'recent',\n",
    "            'count': 100,\n",
    "            'lang': 'en',\n",
    "            'max_id': id_,\n",
    "            'entities': {\n",
    "                \"hashtags\": [],\n",
    "                \"symbols\": [],\n",
    "                \"user_mentions\": []\n",
    "            }\n",
    "            }\n",
    "    #build our query into a dictionary to easily turn into dataframe \n",
    "    dict_ = {'user': [], 'user_id':[], 'post_id': [], 'text': [], 'favorite_count': [], 'hashtags':[], 'symbols':[], 'user_mentions':[], \n",
    "             'retweet_count':[]}\n",
    "    for status in python_tweets.search(**query)['statuses']:\n",
    "        dict_['user'].append(status['user']['screen_name'])\n",
    "        dict_['user_id'].append(status['user']['id'])\n",
    "        dict_['text'].append(status['text'])\n",
    "        dict_['hashtags'].append(status['entities']['hashtags'])\n",
    "        dict_['symbols'].append(status['entities']['symbols'])\n",
    "        dict_['user_mentions'].append(status['entities']['user_mentions'])\n",
    "        dict_['favorite_count'].append(status['favorite_count'])\n",
    "        dict_['retweet_count'].append(status['retweet_count'])\n",
    "        dict_['post_id'].append(status['id'])\n",
    "    # put data in a DataFrame to work with it easier\n",
    "    df = pd.DataFrame(dict_)\n",
    "    #removing our vairables stuck in dictionaries within our dataframe and give them their own columns\n",
    "    df['mentions'] = ' '\n",
    "    df['hashtag'] = ' '\n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(df.at[i, 'user_mentions'])):\n",
    "            try:\n",
    "                df.at[i, 'mentions'] = str(df.at[i, 'mentions']) + ' ' + str(df.at[i, 'user_mentions'][j]['id'])\n",
    "            except:\n",
    "                df.at[i, 'mentions'] = str(df.at[i, 'mentions'])\n",
    "        for k in range(len(df.at[i, 'hashtags'])):\n",
    "            try:\n",
    "                df.at[i, 'hashtag'] = str(df.at[i, 'hashtag']) + ' ' + str(df.at[i, 'hashtags'][k]['text'])\n",
    "            except:\n",
    "                df.at[i, 'hashtag'] = str(df.at[i, 'hashtag'])\n",
    "    # get rid of columns that we took everything we need from \n",
    "    df = df.drop(['user_mentions', 'hashtags'], 1)\n",
    "    startdf = startdf.append(df)\n",
    "    # not sure why id_ doesn't want to change unless reset but do it because it works and can't hurt\n",
    "    id_ = None\n",
    "    # get oldest id_ (lowest number) and subtract one so that that you don't reinclude it in the next search\n",
    "    # reset id_ to be oldest\n",
    "    id_ = int(df['post_id'].sort_values(ascending=True).to_frame().reset_index()['post_id'][0] - 1) \n",
    "    return startdf, id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list1 = ['EXO', 'spacehotel', 'hubble', 'spaceprogram', 'thesolarsystem',\n",
    "              'PerseveranceRover', 'SpaceExploration', 'lifeinspace',\n",
    "              'voyagerstation', 'exoplanet', 'space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list0 = ['ExoMars', 'HubbleGotchu', 'JWST', 'Mars', 'Mars2021',\n",
    "              'PerseveranceRover', 'SpaceExploration', 'astrophotography',\n",
    "              'perseverance', 'exoplanet', 'space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = ['SpaceExploration', 'perseverance', 'exoplanet', 'space', 'EXO', 'spacehotel',\n",
    "               'hubble', 'spaceprogram', 'thesolarsystem', 'lifeinspace', 'voyagerstation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting EXO time : 09:22:16, ending time: 10:52:16\n",
      "starting spacehotel time : 10:52:17, ending time: 12:22:17\n",
      "starting hubble time : 12:22:18, ending time: 13:52:18\n",
      "starting spaceprogram time : 13:52:19, ending time: 15:22:19\n",
      "starting thesolarsystem time : 15:22:20, ending time: 16:52:20\n",
      "starting PerseveranceRover time : 16:52:21, ending time: 18:22:21\n",
      "starting SpaceExploration time : 18:22:22, ending time: 19:52:22\n",
      "starting lifeinspace time : 19:52:23, ending time: 21:22:23\n",
      "starting voyagerstation time : 21:22:24, ending time: 22:52:24\n",
      "starting exoplanet time : 22:52:25, ending time: 00:22:25\n",
      "starting space time : 00:22:26, ending time: 01:52:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tlipman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    }
   ],
   "source": [
    "# eventual goal is to use those loop to do all of our searches\n",
    "end_item = datetime.datetime.now()\n",
    "\n",
    "for item in query_list1:\n",
    "    #reset id_ to be most recent tweet for each item\n",
    "    id_ = python_tweets.search(**{'q': 'RT', 'result_type': 'recent', 'count': 2})\n",
    "    id_ = id_['statuses'][0]['id']\n",
    "    # for each item, reset the start time to the end of the last item \n",
    "    start_time = end_item + datetime.timedelta(seconds=1)\n",
    "    # set the end time to be 90 min after you start, gathering 100 tweets every 100 seconds for 2,700 tweets from each item in list\n",
    "    end_item = end_item + datetime.timedelta(seconds=5401)\n",
    "    #print start and end times so I know when the program will be finished and where it should be at \n",
    "    print(f'starting {item} time : {start_time.strftime(\"%H:%M:%S\")}, ending time: {end_item.strftime(\"%H:%M:%S\")}')\n",
    "    #occassionally getting key/value errors of 0 that I cannot find the cause of, but do not affect anything in how our dataframe\n",
    "    #is made, I just don't want them printing \n",
    "    with suppress(KeyError, ValueError):\n",
    "        #initiate scheduler\n",
    "        sch = Scheduler()\n",
    "        #add our function and items, we're doing 50/10 seconds \n",
    "        sch.add_job(myfn, 'interval', (item, startdf), seconds=10, start_date=start_time, end_date=end_item)\n",
    "        #start scheduler\n",
    "        sch.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to check how many lines of code the dataframe is creating\n",
    "len(startdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                           246266\n",
       "  SpacehotelsTwitterDisco                                    8283\n",
       "  WomenInScience SpaceExploration                            6676\n",
       "  PiDay                                                      3557\n",
       "  OnThisDay                                                  3009\n",
       "  백현 BAEKHYUN EXO 엑소                                         2958\n",
       "  OTD Gagarin                                                2574\n",
       "  Gagarin Vostok1                                            2571\n",
       "  DYK MarsEra USSR Mars6                                     2380\n",
       "  EXO Baekhyun NCT Doyoung                                   2183\n",
       "  NASA SpaceTravel IMAGINEERS ManInSpace                     2168\n",
       "  EXO                                                        1914\n",
       "  spacehotel                                                 1803\n",
       "  InternationalWomensDay                                     1763\n",
       "  SpaceFactFriday                                            1632\n",
       "  MessierMarathon                                            1534\n",
       "  Queen                                                      1224\n",
       "  BAEKHYUN                                                   1169\n",
       "  SpaceHotel                                                 1082\n",
       "  TheDrYve                                                   1080\n",
       "  Sehun                                                      1076\n",
       "  महर्षिदयानंद_का_अज्ञान                                     1047\n",
       "  Blur                                                        846\n",
       "  FridayFeeling                                               834\n",
       "  EXO Baekhyun NCT Doyoung Doll                               754\n",
       "  Spacehotel                                                  716\n",
       "  EXO 엑소                                                      710\n",
       "  MondayMotivation IndianArmy StrongAndCapable                696\n",
       "  Tigray                                                      651\n",
       "  engineers                                                   612\n",
       "  SpaceExploration                                            612\n",
       "  Auschwitz                                                   605\n",
       "  EXOFREEDOM EXOLEAVINGSM                                     595\n",
       "  MachineLearning                                             552\n",
       "  5stars alienromance SciFi SFR worldbuilding oneclick        542\n",
       "  DepecheMode                                                 541\n",
       "  IWD2021                                                     537\n",
       "  HadiahLightstickDariShopee HadiahLightstickDariShopee       509\n",
       "  Flashback                                                   507\n",
       "  백현 엑소 EXO                                                   476\n",
       "  Globur scifi military SF SFF                                476\n",
       "  spaceexploration                                            473\n",
       "  womeninspace spaceexploration WomensHistoryMonth            470\n",
       "  space hotel vacation voyagerstation gatewayfoundation       464\n",
       "  Space                                                       426\n",
       "  StanLee TheMugsOfSpacehotel Marvel                          420\n",
       "  WomenInScience                                              412\n",
       "  outerspace                                                  408\n",
       "  spacex nasa space elonmusk                                  407\n",
       "  나의빛이되어준_진진_생일축하해                                            404\n",
       "Name: hashtag, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used to view the most popular hashtags\n",
    "startdf['hashtag'].value_counts().nlargest(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     RT @MichaelGalanin: A collection of spectacula...\n",
       "1     RT @MichaelGalanin: A collection of spectacula...\n",
       "2     RT @MichaelGalanin: A collection of spectacula...\n",
       "3       ExoMars goes for a spin https://t.co/lsZkrvNmo6\n",
       "4     RT @MichaelGalanin: A collection of spectacula...\n",
       "5     RT @MichaelGalanin: A collection of spectacula...\n",
       "6     RT @MichaelGalanin: A collection of spectacula...\n",
       "7     RT @MichaelGalanin: A collection of spectacula...\n",
       "8     RT @MichaelGalanin: A collection of spectacula...\n",
       "9     RT @MichaelGalanin: A collection of spectacula...\n",
       "10    New session: Perseverance ⟷ ExoMars TGO\\nExpec...\n",
       "11    RT @MichaelGalanin: A collection of spectacula...\n",
       "12    RT @MichaelGalanin: A collection of spectacula...\n",
       "13    ExoMars TGO downlinking 50.85 MB from Curiosit...\n",
       "14    RT @MichaelGalanin: A collection of spectacula...\n",
       "15    RT @MichaelGalanin: A collection of spectacula...\n",
       "16    RT @MichaelGalanin: A collection of spectacula...\n",
       "17    RT @MichaelGalanin: A collection of spectacula...\n",
       "18    RT @MichaelGalanin: A collection of spectacula...\n",
       "19    RT @MichaelGalanin: A collection of spectacula...\n",
       "20    @Astro_Fonseca @ExoMars_CaSSIS @ESA_TGO Comple...\n",
       "21    RT @MichaelGalanin: A collection of spectacula...\n",
       "22    RT @mrn_status: ExoMars TGO downlinking 84.76 ...\n",
       "23    RT @rocdocmars: Sent! Godspeed to my proposal ...\n",
       "24    RT @MichaelGalanin: A collection of spectacula...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tlipman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "Job \"myfn (trigger: interval[0:00:10], next run at: 2021-03-07 18:12:47 EST)\" raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tlipman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 4730, in get_value\n",
      "    return self._engine.get_value(s, k, tz=getattr(series.dtype, \"tz\", None))\n",
      "  File \"pandas/_libs/index.pyx\", line 80, in pandas._libs.index.IndexEngine.get_value\n",
      "  File \"pandas/_libs/index.pyx\", line 88, in pandas._libs.index.IndexEngine.get_value\n",
      "  File \"pandas/_libs/index.pyx\", line 131, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 992, in pandas._libs.hashtable.Int64HashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 998, in pandas._libs.hashtable.Int64HashTable.get_item\n",
      "KeyError: 0\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tlipman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/apscheduler/executors/base.py\", line 125, in run_job\n",
      "    retval = job.func(*job.args, **job.kwargs)\n",
      "  File \"<ipython-input-11-165b5279b83e>\", line 60, in myfn\n",
      "    id_ = int(df['post_id'].sort_values(ascending=True).to_frame().reset_index()['post_id'][0] - 1)\n",
      "  File \"/Users/tlipman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/series.py\", line 1071, in __getitem__\n",
      "    result = self.index.get_value(self, key)\n",
      "  File \"/Users/tlipman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 4736, in get_value\n",
      "    return libindex.get_value_box(s, key)\n",
      "  File \"pandas/_libs/index.pyx\", line 51, in pandas._libs.index.get_value_box\n",
      "  File \"pandas/_libs/index.pyx\", line 47, in pandas._libs.index.get_value_at\n",
      "  File \"pandas/_libs/util.pxd\", line 98, in pandas._libs.util.get_value_at\n",
      "  File \"pandas/_libs/util.pxd\", line 89, in pandas._libs.util.validate_indexer\n",
      "IndexError: index out of bounds\n"
     ]
    }
   ],
   "source": [
    "startdf['text'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'startdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a01d6ab65c41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstartdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'startdf' is not defined"
     ]
    }
   ],
   "source": [
    "startdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since every time we loop through it starts a new index, there are now multiple entries for each index, we need to fix this\n",
    "# and the easiest way is to just reset the index once it finishes running \n",
    "startdf = startdf.drop_duplicates(subset='post_id')\n",
    "startdf = startdf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need these columns anymore\n",
    "startdf = startdf.drop(['index', '', 'symbols'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to add our strings for our hashtags and mentions together we had to make the entire column a string, sinlucding empty cells\n",
    "# here we go through and replace empty cells with nan values so pandas will read them as being empty instead of a string\n",
    "for i in range(len(startdf)):\n",
    "    if startdf.at[i, 'mentions'] == ' ':\n",
    "        startdf.at[i, 'mentions'] = np.nan\n",
    "    else: \n",
    "        pass \n",
    "    if startdf.at[i, 'hashtag'] == ' ':\n",
    "        startdf.at[i, 'hashtag'] = np.nan\n",
    "    else: \n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdf.to_csv(r'C:\\Users\\tlipman\\Documents\\Flatiron_School\\Notes\\Phase4\\tweets_secondary.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
