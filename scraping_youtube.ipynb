{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time \n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials from json file\n",
    "with open(\"/Users/tlipman/.secret/youtube_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(vid_id, page):\n",
    "    \"\"\"Get response request for comments of a YouTube video from the Google API with given video ID and page ID\"\"\"\n",
    "    #setting up API auth\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    api_key = creds['CONSUMER_KEY']\n",
    "    \n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey = api_key)\n",
    "    #setting up our request\n",
    "    req = youtube.commentThreads().list(\n",
    "        part=\"snippet,replies\",\n",
    "        videoId=vid_id,\n",
    "        maxResults= 100,\n",
    "        pageToken= page)\n",
    "    # getting response \n",
    "    response = req.execute()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(num_comments, video_id):\n",
    "    \"\"\"Pass in number of comments requested & the video_id (end of url excluding timestamp), \n",
    "    returns requested comments as pandas DataFrame\"\"\"\n",
    "    # figuring out how many iterations are needed from number of comments requested\n",
    "    iterations = int(num_comments/100)\n",
    "    # setting up our dictionary we will later convert to DataFrame\n",
    "    dict_ = {'text': [], 'vid_id':[], 'likes': [], 'date': [], 'channel_id': [],\n",
    "             'viewer_rating':[], 'mentions':[], 'comment_id':[]\n",
    "    }\n",
    "    # later on we will check if the dataframe is still empty (like we are assigning it here) or if we need to add to a created one\n",
    "    df_youtube = ''\n",
    "    # when starting we do not have a next page, but we will re-write this as we iterate \n",
    "    nextpage = None\n",
    "    # start iterating\n",
    "    for i in range(iterations):\n",
    "        #wrap it in a try statment so that if we ask for too many comments and get an error it stops & still returns our df\n",
    "        try:\n",
    "            # get reponse for page \n",
    "            comments = get_response(video_id, nextpage)\n",
    "            count = 0 \n",
    "            # getting our coments and pages from our response \n",
    "            for item in comments.values():\n",
    "                count = count + 1 \n",
    "                if count == 3:\n",
    "                    nextpage = item\n",
    "                if count == 5:\n",
    "                    comments = item\n",
    "            for i in range(len(comments)):\n",
    "                # extracting data wanted from comments \n",
    "                dict_['text'].append(comments[i]['snippet']['topLevelComment']['snippet']['textOriginal'])\n",
    "                dict_['vid_id'].append(comments[i]['snippet']['videoId'])\n",
    "                dict_['likes'].append(comments[i]['snippet']['topLevelComment']['snippet']['likeCount'])\n",
    "                dict_['date'].append(comments[i]['snippet']['topLevelComment']['snippet']['publishedAt'])\n",
    "                dict_['channel_id'].append(comments[i]['snippet']['topLevelComment']['snippet']['authorChannelId']['value'])\n",
    "                dict_['viewer_rating'].append(comments[i]['snippet']['topLevelComment']['snippet']['viewerRating'])\n",
    "                dict_['comment_id'].append(comments[i]['id'])\n",
    "                dict_['mentions'].append('')\n",
    "                # getting the number of replies \n",
    "                try:\n",
    "                    replies = len(comments[i]['replies'])\n",
    "                except:\n",
    "                    replies = 0\n",
    "                # iterating over our replies and adding them to our dataframe \n",
    "                for j in range(replies):\n",
    "                    dict_['text'].append(comments[i]['replies']['comments'][j]['snippet']['textOriginal'])\n",
    "                    dict_['vid_id'].append(comments[i]['replies']['comments'][j]['snippet']['videoId'])\n",
    "                    dict_['likes'].append(comments[i]['replies']['comments'][j]['snippet']['likeCount'])\n",
    "                    dict_['date'].append(comments[i]['replies']['comments'][j]['snippet']['publishedAt'])\n",
    "                    dict_['channel_id'].append(comments[i]['replies']['comments'][j]['snippet']['authorChannelId']['value'])\n",
    "                    dict_['viewer_rating'].append(comments[i]['replies']['comments'][j]['snippet']['viewerRating'])              \n",
    "                    dict_['mentions'].append(comments[i]['replies']['comments'][j]['snippet']['parentId'])\n",
    "                    dict_['comment_id'].append(comments[i]['replies']['comments'][j]['id'])\n",
    "        except:\n",
    "            pass\n",
    "    # converting our dictionary into a DataFrame and returning it \n",
    "    df_youtube = pd.DataFrame.from_dict(dict_)\n",
    "    return df_youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictioniary of videos to get the comments from \n",
    "video_dict1 = {\n",
    "    'Spaceship Design': 'V4ddnrBT6hE',\n",
    "    'The Race to Mars in 2020': 'VQY3qWZMIl8',\n",
    "    'What If We Try And Colonize Mars?': 'Gcnf5BdLXxw',\n",
    "    'The VASIMR Engine: How to Get to Mars in 40 Days': 'uqX8wIkjoYg',\n",
    "    'Bigelow Aerospace is Building the Worlds First Hotel | Answers with Joe': '5nE3UO1kqv0',\n",
    "    'Why Earth Is A Prison and How To Escape It': 'RVMZxH1TIIQ',\n",
    "    'Five REAL Possibilities for Interstellar Travel': 'EzZGPCyrpSU',\n",
    "    'Horizons mission - Soyuz: launch to orbit': 'fr_hXLDLc38',\n",
    "    'Will Humanity Reach Another Star In Your Lifetime?': '3zMUJwGrn6Q'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict2 = {'How Fast Can We Travel In Space?': 'OYAgcS31-p0',\n",
    "        'What I learned from going blind in space | Chris Hadfield': 'Zo62S0ulqhA',\n",
    "        'Three Men Lost in Space – The Apollo 13 Disaster': '5OLtteIwwNs',\n",
    "        'How Far Can We Go? Limits of Humanity.': 'ZL4yYHdDSWs',\n",
    "        '10 Terrifying Facts About Space': 'UNEFDynNw-Q',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict3 = {'Perseverance Rover’s Descent and Touchdown on Mars (Official NASA Video)': '4czjS9h4Fpg',\n",
    "               'NASA’S Perseverance Rover’s First 360 View of Mars (Official)': 'wE-aQO9XD1g',\n",
    "               'Perseverance’s First Sounds from Mars': 'ZBFjpnV9-sg',\n",
    "               'Curiosity Mars Rover Snaps 1.8 Billion-Pixel Panorama (narrated video)': 'X2UaFuJsqxk',\n",
    "               'The Mars Homes That NASA Awarded $500k': 'LCuZC-CRg4M',\n",
    "               'An Epic Journey to a Black Hole to Give You Goosebumps': 'RV170sqhm4Q',\n",
    "               'Mars: What happened to the Spirit Rover?': '7V54LRRJaGk',\n",
    "               'Preparing to Land Perseverance': 'v7iUb_wDHxk',\n",
    "               'Testing the Curiosity Rover on Earth': '3-MNAX1jgbA',\n",
    "               'HASSELL + EOC presents MARS HABITAT': 'AIrH01N9AsE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V4ddnrBT6hE',\n",
       " 'VQY3qWZMIl8',\n",
       " 'Gcnf5BdLXxw',\n",
       " 'uqX8wIkjoYg',\n",
       " '5nE3UO1kqv0',\n",
       " 'RVMZxH1TIIQ',\n",
       " 'EzZGPCyrpSU',\n",
       " 'fr_hXLDLc38',\n",
       " '3zMUJwGrn6Q']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list = []\n",
    "for value in video_dict1.values():\n",
    "    video_list.append(value)\n",
    "video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V4ddnrBT6hE',\n",
       " 'VQY3qWZMIl8',\n",
       " 'Gcnf5BdLXxw',\n",
       " 'uqX8wIkjoYg',\n",
       " '5nE3UO1kqv0',\n",
       " 'RVMZxH1TIIQ',\n",
       " 'EzZGPCyrpSU',\n",
       " 'fr_hXLDLc38',\n",
       " '3zMUJwGrn6Q',\n",
       " 'OYAgcS31-p0',\n",
       " 'Zo62S0ulqhA',\n",
       " '5OLtteIwwNs',\n",
       " 'ZL4yYHdDSWs',\n",
       " 'UNEFDynNw-Q']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for value in video_dict2.values():\n",
    "    video_list.append(value)\n",
    "video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V4ddnrBT6hE',\n",
       " 'VQY3qWZMIl8',\n",
       " 'Gcnf5BdLXxw',\n",
       " 'uqX8wIkjoYg',\n",
       " '5nE3UO1kqv0',\n",
       " 'RVMZxH1TIIQ',\n",
       " 'EzZGPCyrpSU',\n",
       " 'fr_hXLDLc38',\n",
       " '3zMUJwGrn6Q',\n",
       " 'OYAgcS31-p0',\n",
       " 'Zo62S0ulqhA',\n",
       " '5OLtteIwwNs',\n",
       " 'ZL4yYHdDSWs',\n",
       " 'UNEFDynNw-Q',\n",
       " '4czjS9h4Fpg',\n",
       " 'wE-aQO9XD1g',\n",
       " 'ZBFjpnV9-sg',\n",
       " 'X2UaFuJsqxk',\n",
       " 'LCuZC-CRg4M',\n",
       " 'RV170sqhm4Q',\n",
       " '7V54LRRJaGk',\n",
       " 'v7iUb_wDHxk',\n",
       " '3-MNAX1jgbA',\n",
       " 'AIrH01N9AsE']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for value in video_dict3.values():\n",
    "    video_list.append(value)\n",
    "video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting a dataframe for us to add additional comments onto \n",
    "df = get_comments(100, 'BI-old7YI4I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterating over our list of videos to get their comments and add them to our starting dataframe \n",
    "for video in video_list:\n",
    "    df = df.append(get_comments(100, video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of duplicate entries (if any)\n",
    "df.drop_duplicates(subset='comment_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('youtube_march28', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
